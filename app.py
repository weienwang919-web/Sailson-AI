import os
import sys
import datetime
import time
import logging
import pandas as pd
import uuid
import threading
from io import BytesIO
from PIL import Image
from flask import Flask, render_template, request, jsonify, redirect, url_for, session, send_file
from flask_bcrypt import Bcrypt
from apify_client import ApifyClient
from dotenv import load_dotenv
from openai import OpenAI
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from functools import wraps
import database as db

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ============================================
# æ—¥å¿—é…ç½® - ç¡®ä¿è¾“å‡ºåˆ° stdout
# ============================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ============================================
# ç¯å¢ƒé…ç½® - ä¼˜åŒ–ç‰ˆ
# ============================================

# æ¸…é™¤ä»£ç†è®¾ç½®ï¼ˆäº‘ç«¯ç¯å¢ƒä¸éœ€è¦ä»£ç†ï¼‰
for proxy_var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    if proxy_var in os.environ:
        del os.environ[proxy_var]
        logger.info(f"ğŸ§¹ å·²æ¸…é™¤ä»£ç†è®¾ç½®: {proxy_var}")

# åŠ è½½ç¯å¢ƒå˜é‡
DASHSCOPE_API_KEY = os.environ.get('DASHSCOPE_API_KEY')
APIFY_TOKEN = os.environ.get('APIFY_TOKEN')
PORT = int(os.environ.get('PORT', 5001))

# å¯åŠ¨æ—¶è¾“å‡ºé…ç½®çŠ¶æ€
logger.info("=" * 60)
logger.info("ğŸš€ Sailson AI å·¥ä½œå°å¯åŠ¨ä¸­...")
logger.info(f"ğŸ”‘ DASHSCOPE_API_KEY: {'âœ… å·²é…ç½®' if DASHSCOPE_API_KEY else 'âŒ æœªé…ç½®'}")
logger.info(f"ğŸ”‘ APIFY_TOKEN: {'âœ… å·²é…ç½®' if APIFY_TOKEN else 'âŒ æœªé…ç½®'}")
logger.info(f"ğŸŒ PORT: {PORT}")
logger.info(f"ğŸ Python ç‰ˆæœ¬: {sys.version}")
logger.info("=" * 60)

# åˆå§‹åŒ– AI å¼•æ“
if DASHSCOPE_API_KEY:
    try:
        qwen_client = OpenAI(
            api_key=DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        logger.info("âœ… é€šä¹‰åƒé—® API åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ é€šä¹‰åƒé—® API åˆå§‹åŒ–å¤±è´¥: {e}")
        qwen_client = None
else:
    logger.warning("âš ï¸ è­¦å‘Š: DASHSCOPE_API_KEY æœªé…ç½®ï¼ŒAI åŠŸèƒ½å°†ä¸å¯ç”¨")
    qwen_client = None

# åˆå§‹åŒ–çˆ¬è™«å¼•æ“
if APIFY_TOKEN:
    try:
        apify_client = ApifyClient(APIFY_TOKEN)
        logger.info("âœ… Apify å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Apify å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        apify_client = None
else:
    logger.warning("âš ï¸ è­¦å‘Š: APIFY_TOKEN æœªé…ç½®ï¼Œçˆ¬è™«åŠŸèƒ½å°†ä¸å¯ç”¨")
    apify_client = None

# Flask åº”ç”¨åˆå§‹åŒ–
app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'sailson_secure_key')
bcrypt = Bcrypt(app)

# å†…å­˜å­˜å‚¨ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
HISTORY_DB = []
LATEST_ANALYSIS_RESULTS = {}  # å­˜å‚¨æœ€æ–°çš„åˆ†æç»“æœï¼Œç”¨äºå¯¼å‡º
# TASK_QUEUE å·²è¿ç§»åˆ°æ•°æ®åº“ï¼Œä¸å†ä½¿ç”¨å†…å­˜å­—å…¸

# æ±‡ç‡é…ç½®
USD_TO_CNY = 7.2

# ============================================
# ä»»åŠ¡æ¢å¤æœºåˆ¶ï¼ˆå®šä¹‰ï¼Œç¨åè°ƒç”¨ï¼‰
# ============================================

def recover_interrupted_tasks():
    """æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡"""
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ processing çŠ¶æ€çš„ä»»åŠ¡ï¼ˆè¯´æ˜è¢«ä¸­æ–­äº†ï¼‰
        interrupted_tasks = db.query_all("""
            SELECT task_id FROM task_queue
            WHERE status = 'processing'
            AND created_at > NOW() - INTERVAL '1 hour'
        """)

        if interrupted_tasks:
            logger.warning(f"âš ï¸ å‘ç° {len(interrupted_tasks)} ä¸ªè¢«ä¸­æ–­çš„ä»»åŠ¡ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
            for task in interrupted_tasks:
                update_task(
                    task['task_id'],
                    status='failed',
                    error='æœåŠ¡é‡å¯å¯¼è‡´ä»»åŠ¡ä¸­æ–­',
                    progress='ä»»åŠ¡å·²ä¸­æ–­'
                )
    except Exception as e:
        logger.error(f"âŒ æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")

# ============================================
# è£…é¥°å™¨ï¼šæƒé™æ§åˆ¶
# ============================================

def login_required(f):
    """éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def admin_required(f):
    """éœ€è¦ç®¡ç†å‘˜æƒé™æ‰èƒ½è®¿é—®"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        if session.get('role') != 'admin':
            return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403
        return f(*args, **kwargs)
    return decorated_function

# ============================================
# æ ¸å¿ƒå·¥å…·å‡½æ•°
# ============================================

def create_task(task_id, user_id, session_id):
    """åˆ›å»ºä»»åŠ¡è®°å½•"""
    try:
        db.execute("""
            INSERT INTO task_queue (task_id, user_id, session_id, status, progress)
            VALUES (%s, %s, %s, %s, %s)
        """, (task_id, user_id, session_id, 'pending', 'ä»»åŠ¡å·²åˆ›å»º'))
        logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²å†™å…¥æ•°æ®åº“")
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")

def update_task(task_id, status=None, progress=None, result=None, error=None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    try:
        updates = []
        params = []

        if status is not None:
            updates.append("status = %s")
            params.append(status)
        if progress is not None:
            updates.append("progress = %s")
            params.append(progress)
        if result is not None:
            updates.append("result = %s")
            params.append(result)
        if error is not None:
            updates.append("error = %s")
            params.append(error)

        if updates:
            updates.append("updated_at = CURRENT_TIMESTAMP")
            params.append(task_id)

            sql = f"UPDATE task_queue SET {', '.join(updates)} WHERE task_id = %s"
            db.execute(sql, tuple(params))
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")

def get_task(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        task = db.query_one("""
            SELECT task_id, status, progress, result, error
            FROM task_queue
            WHERE task_id = %s
        """, (task_id,))
        return task
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return None

# ============================================
# å¯åŠ¨æ—¶æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡
# ============================================
recover_interrupted_tasks()


def call_gemini(prompt, image=None, timeout=60):
    """è°ƒç”¨é€šä¹‰åƒé—® API"""
    if not qwen_client:
        error_msg = "âŒ é”™è¯¯ï¼šDASHSCOPE_API_KEY æœªé…ç½®"
        logger.error(error_msg)
        return error_msg, 0

    model_name = 'qwen-turbo'

    try:
        logger.info(f"ğŸ¤– æ­£åœ¨è°ƒç”¨é€šä¹‰åƒé—®æ¨¡å‹: {model_name}")
        logger.info(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

        response = qwen_client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )

        result = response.choices[0].message.content
        tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0
        logger.info(f"âœ… é€šä¹‰åƒé—®è°ƒç”¨æˆåŠŸï¼Œè¿”å› {len(result)} å­—ç¬¦ï¼Œæ¶ˆè€— {tokens} tokens")
        return result, tokens

    except Exception as e:
        error_msg = f"âš ï¸ é€šä¹‰åƒé—® API è°ƒç”¨å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg, 0


def process_uploaded_file(file_data):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå›¾ç‰‡æˆ–è¡¨æ ¼ï¼‰

    Args:
        file_data: å­—å…¸ï¼ŒåŒ…å« filename, content, content_type
    """
    try:
        fname = file_data['filename'].lower()
        content = file_data['content']
        logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {fname}")

        if fname.endswith(('.png', '.jpg', '.jpeg', '.webp')):
            logger.info("ğŸ–¼ï¸ è¯†åˆ«ä¸ºå›¾ç‰‡æ–‡ä»¶")
            from io import BytesIO
            return "IMAGE", Image.open(BytesIO(content))

        if fname.endswith(('.xlsx', '.csv')):
            logger.info("ğŸ“Š è¯†åˆ«ä¸ºè¡¨æ ¼æ–‡ä»¶")
            from io import BytesIO
            if fname.endswith('.csv'):
                df = pd.read_csv(BytesIO(content))
            else:
                df = pd.read_excel(BytesIO(content))
            return "TEXT", df.to_string(index=False, max_rows=50)

        return "ERROR", "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"

    except Exception as e:
        error_msg = f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
        logger.info(f"âŒ {error_msg}")
        return "ERROR", error_msg


def save_history(title, result, type_tag):
    """ä¿å­˜åˆ°å†å²è®°å½•ï¼ˆæ•°æ®åº“ï¼‰"""
    try:
        user_id = session.get('user_id')
        if not user_id:
            logger.warning("âš ï¸ æœªç™»å½•ç”¨æˆ·ï¼Œè·³è¿‡ä¿å­˜å†å²è®°å½•")
            return

        # ä¿å­˜åˆ°æ•°æ®åº“
        db.execute("""
            INSERT INTO analysis_results (user_id, title, result, type)
            VALUES (%s, %s, %s, %s)
        """, (user_id, title, result, type_tag))

        logger.info(f"ğŸ’¾ å·²ä¿å­˜å†å²è®°å½•åˆ°æ•°æ®åº“: {title}")

        # åŒæ—¶ä¿å­˜åˆ°å†…å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
        record = {
            'id': len(HISTORY_DB) + 1,
            'title': f"{title} [{datetime.datetime.now().strftime('%H:%M')}]",
            'result': result,
            'type': type_tag
        }
        HISTORY_DB.append(record)

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è‡³å°‘ä¿å­˜åˆ°å†…å­˜
        record = {
            'id': len(HISTORY_DB) + 1,
            'title': f"{title} [{datetime.datetime.now().strftime('%H:%M')}]",
            'result': result,
            'type': type_tag
        }
        HISTORY_DB.append(record)


def call_veo_api(prompt):
    """è°ƒç”¨ Google Veo APIï¼ˆæ¨¡æ‹Ÿï¼‰"""
    logger.info(f"ğŸ¬ æ¨¡æ‹Ÿ Veo API è°ƒç”¨: {prompt[:50]}...")
    time.sleep(3)
    return "https://cdn.pixabay.com/video/2023/10/22/186115-877653483_large.mp4"


def log_usage(user_id, username, department, function_type, comments_count, ai_tokens):
    """è®°å½•ä½¿ç”¨æƒ…å†µå’Œæˆæœ¬"""
    try:
        # è®¡ç®—æˆæœ¬
        ai_cost = ai_tokens * 0.008 / 1000  # é€šä¹‰åƒé—®å®šä»·

        # æ ¹æ®åŠŸèƒ½ç±»å‹è®¡ç®— Apify æˆæœ¬
        if function_type == 'sentiment':
            # Facebook è¯„è®ºï¼š$2.50/1000æ¡
            apify_cost_usd = comments_count * 2.50 / 1000
        elif function_type == 'competitor':
            # TikTok æ•°æ®ï¼š$3.70/1000æ¡
            apify_cost_usd = comments_count * 3.70 / 1000
        else:
            apify_cost_usd = 0

        apify_cost = apify_cost_usd * USD_TO_CNY  # è½¬æ¢ä¸ºäººæ°‘å¸
        total_cost = ai_cost + apify_cost

        # ä¿å­˜åˆ°æ•°æ®åº“
        db.execute("""
            INSERT INTO usage_logs
            (user_id, username, department, function_type, comments_count,
             ai_tokens, ai_cost, apify_cost, total_cost)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (user_id, username, department, function_type, comments_count,
              ai_tokens, ai_cost, apify_cost, total_cost))

        logger.info(f"ğŸ’° æˆæœ¬è®°å½•: AI={ai_cost:.4f}å…ƒ + Apify={apify_cost:.4f}å…ƒ = æ€»è®¡{total_cost:.4f}å…ƒ")

        return total_cost

    except Exception as e:
        logger.error(f"âŒ è®°å½•ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
        return 0


def process_analysis_task(task_id, url, file_data, session_id, user_id, username, department):
    """å¼‚æ­¥å¤„ç†åˆ†æä»»åŠ¡"""
    # ç”¨æˆ·ä¿¡æ¯å·²ä»ä¸»çº¿ç¨‹ä¼ å…¥ï¼Œä¸å†ä» session è·å–

    logger.info(f"ğŸ”„ åå°çº¿ç¨‹å·²å¯åŠ¨ï¼Œä»»åŠ¡ID: {task_id}")
    logger.info(f"ğŸ‘¤ ç”¨æˆ·ä¿¡æ¯: user_id={user_id}, username={username}, department={department}")
    logger.info(f"ğŸ“‹ ä»»åŠ¡å‚æ•°: url={url}, has_file={file_data is not None}")

    # è¿½è¸ªæˆæœ¬æ•°æ®
    total_tokens = 0
    total_comments = 0

    try:
        logger.info(f"ğŸ“ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º processing...")
        update_task(task_id, status='processing', progress='æ­£åœ¨åˆå§‹åŒ–...')
        logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€æ›´æ–°æˆåŠŸ")

        content = ""
        img = None
        source_title = "æœªçŸ¥"

        # è·¯å¾„ A: æ–‡ä»¶ä¸Šä¼ åˆ†æ
        if file_data:
            update_task(task_id, progress='æ­£åœ¨å¤„ç†æ–‡ä»¶...')
            mode, res = process_uploaded_file(file_data)

            if mode == "ERROR":
                update_task(task_id, status='failed', error=res)
                return

            if mode == "IMAGE":
                img = res
                content = "åˆ†æå›¾ç‰‡ä¸­çš„åé¦ˆå†…å®¹"
            else:
                content = res

            source_title = f"æ–‡ä»¶: {file_data.filename[:15]}"

        # è·¯å¾„ B: ç¤¾äº¤åª’ä½“é“¾æ¥æŠ“å–åˆ†æ
        elif url:
            logger.info(f"ğŸŒ å¼€å§‹å¤„ç† URL: {url}")
            update_task(task_id, progress='æ­£åœ¨æŠ“å–ç¤¾åª’æ•°æ®...')

            if not apify_client:
                logger.error("âŒ Apify å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                update_task(task_id, status='failed', error="APIFY_TOKEN æœªé…ç½®")
                return

            try:
                logger.info("ğŸ“‹ å‡†å¤‡ Apify çˆ¬è™«å‚æ•°...")
                run_input = {
                    "startUrls": [{"url": url}],
                    "resultsLimit": 1000,
                    "maxComments": 1000,
                    "maxPostCount": 1,
                    "maxCommentsPerPost": 1000,
                    "maxRepliesPerComment": 0,
                    "scrapeCommentReplies": False
                }

                logger.info("ğŸš€ å¯åŠ¨ Apify çˆ¬è™«...")
                try:
                    run = apify_client.actor("apify/facebook-comments-scraper").start(run_input=run_input)
                    logger.info(f"âœ… çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼ŒRun ID: {run['id']}")
                except Exception as start_error:
                    error_msg = f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(start_error)}"
                    logger.error(f"âŒ {error_msg}")
                    logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(start_error).__name__}")
                    import traceback
                    logger.error(f"âŒ å †æ ˆ:\n{traceback.format_exc()}")
                    update_task(task_id, status='failed', error=error_msg)
                    return

                logger.info("â³ ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆæœ€é•¿ 180 ç§’ï¼‰...")
                update_task(task_id, progress='ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆçº¦30-60ç§’ï¼‰...')
                run = apify_client.run(run['id']).wait_for_finish(wait_secs=180)
                logger.info(f"âœ… çˆ¬è™«å®Œæˆï¼ŒçŠ¶æ€: {run['status']}")

                if run['status'] != 'SUCCEEDED':
                    logger.error(f"âŒ çˆ¬è™«ä»»åŠ¡å¤±è´¥: {run['status']}")
                    update_task(task_id, status='failed', error=f"çˆ¬è™«ä»»åŠ¡å¤±è´¥: {run['status']}")
                    return

                # è·å–æ•°æ®
                logger.info("ğŸ“¦ å¼€å§‹è·å–çˆ¬è™«æ•°æ®...")
                dataset_client = apify_client.dataset(run["defaultDatasetId"])
                items = []
                offset = 0
                limit = 1000

                while True:
                    batch = dataset_client.list_items(offset=offset, limit=limit).items
                    if not batch:
                        break
                    items.extend(batch)
                    if len(batch) < limit:
                        break
                    offset += limit

                logger.info(f"âœ… æ€»å…±è·å–åˆ° {len(items)} æ¡æ•°æ®")
                total_comments = len(items)  # è®°å½•è¯„è®ºæ•°

                if not items:
                    update_task(task_id, status='failed', error="æœªå‘ç°å…¬å¼€è¯„è®º")
                    return

                # åˆ†æ‰¹å¤„ç†è¯„è®º
                batch_size = 50
                all_results = []
                total_batches = (len(items) + batch_size - 1) // batch_size

                for i in range(0, len(items), batch_size):
                    batch = items[i:i+batch_size]
                    batch_num = i // batch_size + 1

                    update_task(task_id, progress=f'AI åˆ†æä¸­ï¼šç¬¬ {batch_num}/{total_batches} æ‰¹...')
                    logger.info(f"ğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ï¼ˆ{len(batch)} æ¡è¯„è®ºï¼‰...")

                    batch_content = "\n".join([f"ç”¨æˆ·{j}: {it.get('text', '')}" for j, it in enumerate(batch)])

                    batch_prompt = f"""
Analyze these comments and categorize them. Output ONLY a JSON array.

Comments:
{batch_content}

Categories (Chinese only):
1. å¤–æŒ‚ä½œå¼Š - hackers, cheating
2. æ¸¸æˆä¼˜åŒ– - lag, crashes
3. æ¸¸æˆBug - glitches, errors
4. å……å€¼é€€æ¬¾ - payment issues
5. æ–°æ¨¡å¼/åœ°å›¾/å¹³è¡¡æ€§å»ºè®® - new content requests
6. å…¶ä»– - spam, praise

Output format (JSON array only, no markdown):
[
  {{{{
    "text": "comment text",
    "category": "å¤–æŒ‚ä½œå¼Š",
    "sentiment": "è´Ÿé¢",
    "language": "è‹±è¯­",
    "analysis": "è¯¦ç»†åˆ†æå†…å®¹"
  }}}},
  ...
]

IMPORTANT:
- Output ONLY valid JSON array
- Skip "å…¶ä»–" category
- Use Chinese for category, sentiment, language, and analysis
- Language options (MUST be one of these): è‹±è¯­, è²å¾‹å®¾è¯­, æ³°è¯­, è¶Šå—è¯­, å°å°¼è¯­, é©¬æ¥è¯­
- Identify the language accurately based on the text
- Analysis requirements:
  * For short comments (< 30 chars): One sentence summary (15-20 Chinese characters)
  * For medium/long comments (>= 30 chars): Detailed analysis (40-50 Chinese characters)
  * Include: main issue, player emotion, key details
"""

                    result, tokens = call_gemini(batch_prompt, timeout=60)
                    total_tokens += tokens

                    # è§£æ JSON ç»“æœ
                    try:
                        import json
                        import re
                        clean_result = re.sub(r'```json\\s*|\\s*```', '', result).strip()
                        batch_data = json.loads(clean_result)
                        all_results.extend(batch_data)
                        logger.info(f"âœ… ç¬¬ {batch_num} æ‰¹å®Œæˆï¼Œè·å¾— {len(batch_data)} æ¡æœ‰æ•ˆç»“æœ")
                    except Exception as e:
                        logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹è§£æå¤±è´¥: {e}")
                        continue

                # ç”Ÿæˆ HTML è¡¨æ ¼
                update_task(task_id, progress='ç”ŸæˆæŠ¥å‘Š...')
                logger.info(f"ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œå…± {len(all_results)} æ¡æœ‰æ•ˆè¯„è®º...")

                # æŒ‰åˆ†ç±»æ’åº
                category_order = ["å¤–æŒ‚ä½œå¼Š", "æ¸¸æˆä¼˜åŒ–", "æ¸¸æˆBug", "å……å€¼é€€æ¬¾", "æ–°æ¨¡å¼/åœ°å›¾/å¹³è¡¡æ€§å»ºè®®"]
                all_results.sort(key=lambda x: category_order.index(x.get('category', 'å…¶ä»–')) if x.get('category') in category_order else 999)

                # ç”Ÿæˆ HTML
                html_rows = []
                for idx, item in enumerate(all_results, 1):
                    html_rows.append(f"""
                    <tr>
                        <td>{idx}</td>
                        <td style="white-space: pre-wrap; word-break: break-word;">{item.get('text', '')}</td>
                        <td><strong>{item.get('category', '')}</strong></td>
                        <td>{item.get('sentiment', '')}</td>
                        <td>{item.get('language', '')}</td>
                        <td style="white-space: pre-wrap; word-break: break-word;">{item.get('analysis', '')}</td>
                    </tr>
                    """)

                result = f"""
                <table class="table table-hover">
                    <thead>
                        <tr>
                            <th style="width:40px;">#</th>
                            <th style="width:25%;">åŸå§‹è¯„è®º</th>
                            <th style="width:100px;">å½’ç±»</th>
                            <th style="width:70px;">æƒ…æ„Ÿ</th>
                            <th style="width:60px;">è¯­è¨€</th>
                            <th>ç®€è¦åˆ†æ</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join(html_rows)}
                    </tbody>
                </table>
                """

                # ä¿å­˜ç»“æœç”¨äºå¯¼å‡º
                LATEST_ANALYSIS_RESULTS[session_id] = all_results
                source_title = f"FB: {url[:15]}..."

            except Exception as e:
                error_msg = f"çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
                import traceback
                logger.error(f"âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")

                update_task(task_id, status='failed', error=error_msg)
                return

        else:
            update_task(task_id, status='failed', error="è¯·æä¾›é“¾æ¥æˆ–æ–‡ä»¶")
            return

        # ä¿å­˜å†å²è®°å½•
        save_history(source_title, result, 'sentiment')

        # è®°å½•ä½¿ç”¨æˆæœ¬
        if user_id:
            log_usage(user_id, username, department, 'sentiment', total_comments, total_tokens)

        # ä»»åŠ¡å®Œæˆ
        update_task(task_id, status='completed', result=result, progress='åˆ†æå®Œæˆï¼')
        logger.info(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ")

    except Exception as e:
        error_msg = f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
        logger.error(f"âŒ ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
        logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")

        update_task(task_id, status='failed', error=error_msg, progress='ä»»åŠ¡å¤±è´¥')

# ============================================
# åŸºç¡€è·¯ç”±
# ============================================

@app.route('/login', methods=['GET', 'POST'])
def login():
    """ç™»å½•é¡µé¢"""
    if request.method == 'POST':
        username = request.form.get('username')
        password = request.form.get('password')

        try:
            # ä»æ•°æ®åº“æŸ¥è¯¢ç”¨æˆ·
            user = db.query_one(
                "SELECT * FROM users WHERE username = %s",
                (username,)
            )

            if user and bcrypt.check_password_hash(user['password_hash'], password):
                session['logged_in'] = True
                session['user_id'] = user['id']
                session['username'] = user['username']
                session['real_name'] = user['real_name']
                session['department'] = user['department']
                session['role'] = user['role']
                session['session_id'] = f"{username}_{int(time.time())}"
                logger.info(f"âœ… ç”¨æˆ·ç™»å½•æˆåŠŸ: {username} ({user['real_name']})")
                return redirect(url_for('home'))
            else:
                logger.info(f"âŒ ç™»å½•å¤±è´¥: {username}")
                return render_template('login.html', error='ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯')

        except Exception as e:
            logger.error(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return render_template('login.html', error='ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•')

    return render_template('login.html')


@app.route('/logout')
def logout():
    """ç™»å‡º"""
    username = session.get('username', 'æœªçŸ¥ç”¨æˆ·')
    session.clear()
    logger.info(f"ğŸ‘‹ ç”¨æˆ·å·²ç™»å‡º: {username}")
    return redirect(url_for('login'))


@app.route('/')
@login_required
def home():
    """é¦–é¡µ"""
    return render_template('index.html', user=session)


@app.route('/dashboard_stats')
@login_required
def dashboard_stats():
    """é¦–é¡µæ•°æ®çœ‹æ¿ API"""
    try:
        current_month = datetime.datetime.now().strftime('%Y-%m')

        # æœ¬æœˆæ•°æ®
        current_data = db.query_one("""
            SELECT
                COALESCE(SUM(comments_count), 0) as total_comments,
                COUNT(*) as total_analyses,
                COALESCE(SUM(ai_tokens), 0) as total_tokens
            FROM usage_logs
            WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
        """, (current_month,))

        # ä¸Šæœˆæ•°æ®ï¼ˆç”¨äºè®¡ç®—å¢é•¿ç‡ï¼‰
        last_month = (datetime.datetime.now().replace(day=1) - datetime.timedelta(days=1)).strftime('%Y-%m')
        last_data = db.query_one("""
            SELECT COALESCE(SUM(comments_count), 0) as total_comments
            FROM usage_logs
            WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
        """, (last_month,))

        # è®¡ç®—å¢é•¿ç‡
        growth = 0
        if last_data and last_data['total_comments'] > 0:
            growth = ((current_data['total_comments'] - last_data['total_comments']) / last_data['total_comments']) * 100

        return jsonify({
            'comments': int(current_data['total_comments']),
            'analyses': int(current_data['total_analyses']),
            'tokens': int(current_data['total_tokens']),
            'growth': round(growth, 1)
        })

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®çœ‹æ¿å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤å€¼
        return jsonify({
            'comments': 0,
            'analyses': 0,
            'tokens': 0,
            'growth': 0
        })


@app.route('/debug')
@login_required
def debug_page():
    """è°ƒè¯•é¡µé¢"""
    debug_info = {
        "status": "Online",
        "qwen_key": bool(DASHSCOPE_API_KEY),
        "apify_key": bool(APIFY_TOKEN),
        "port": PORT,
        "history_count": len(HISTORY_DB)
    }
    logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯: {debug_info}")
    return jsonify(debug_info)


@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ç”¨äº Render ç›‘æ§"""
    return jsonify({"status": "ok", "service": "Sailson AI"}), 200


@app.route('/submit_feedback', methods=['POST'])
def submit_feedback():
    """æ¥æ”¶ç”¨æˆ·åé¦ˆå¹¶å‘é€é‚®ä»¶"""
    try:
        data = request.json
        project_name = data.get('project_name')
        feedback = data.get('feedback')

        if not project_name or not feedback:
            return jsonify({'error': 'è¯·å¡«å†™å®Œæ•´ä¿¡æ¯'}), 400

        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"ğŸ“§ æ”¶åˆ°ç”¨æˆ·åé¦ˆ")
        logger.info(f"   é¡¹ç›®åç§°: {project_name}")
        logger.info(f"   åé¦ˆå†…å®¹: {feedback}")

        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
        try:
            db.execute("""
                INSERT INTO feedback (user_email, content, created_at)
                VALUES (%s, %s, NOW())
            """, (project_name, feedback))
        except Exception as db_error:
            # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåªè®°å½•æ—¥å¿—
            logger.warning(f"âš ï¸ ä¿å­˜åé¦ˆåˆ°æ•°æ®åº“å¤±è´¥ï¼ˆè¡¨å¯èƒ½ä¸å­˜åœ¨ï¼‰: {db_error}")

        # TODO: å‘é€é‚®ä»¶é€šçŸ¥ç®¡ç†å‘˜
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶æœåŠ¡ï¼ˆå¦‚ SendGrid, AWS SESï¼‰
        # send_email(
        #     to="admin@sailson.com",
        #     subject=f"æ–°ç”¨æˆ·åé¦ˆ - {project_name}",
        #     body=feedback
        # )

        return jsonify({'success': True, 'message': 'æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼'})

    except Exception as e:
        logger.error(f"âŒ å¤„ç†åé¦ˆå¤±è´¥: {e}")
        return jsonify({'error': 'ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•'}), 500


# ============================================
# åŠŸèƒ½ 1: èˆ†æƒ…åˆ†æ
# ============================================

@app.route('/sentiment-tool')
@login_required
def sentiment_tool():
    """èˆ†æƒ…åˆ†æå·¥å…·é¡µé¢"""
    return render_template('analysis.html')


@app.route('/analyze', methods=['POST'])
def analyze():
    """èˆ†æƒ…åˆ†æ API - å¼‚æ­¥ç‰ˆæœ¬"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ”¶åˆ°èˆ†æƒ…åˆ†æè¯·æ±‚")
    logger.info(f"ğŸ”‘ DASHSCOPE_API_KEY: {'âœ…' if DASHSCOPE_API_KEY else 'âŒ'}")
    logger.info(f"ğŸ”‘ APIFY_TOKEN: {'âœ…' if APIFY_TOKEN else 'âŒ'}")

    url = request.form.get('url')
    file = request.files.get('file')

    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = str(uuid.uuid4())
    session_id = session.get('session_id', 'default')

    # åœ¨ä¸»çº¿ç¨‹ä¸­æå–ç”¨æˆ·ä¿¡æ¯ï¼ˆé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰
    user_id = session.get('user_id')
    username = session.get('username', 'unknown')
    department = session.get('department', 'æœªçŸ¥')

    # åœ¨ä¸»çº¿ç¨‹ä¸­è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆé¿å…è·¨çº¿ç¨‹è®¿é—® Flask FileStorage å¯¹è±¡ï¼‰
    file_data = None
    if file:
        try:
            file_data = {
                'filename': file.filename,
                'content': file.read(),  # è¯»å–æ–‡ä»¶å†…å®¹åˆ°å†…å­˜
                'content_type': file.content_type
            }
            logger.info(f"ğŸ“ å·²è¯»å–æ–‡ä»¶: {file.filename}, å¤§å°: {len(file_data['content'])} å­—èŠ‚")
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return jsonify({'error': f'è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'}), 400

    # åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
    create_task(task_id, user_id, session_id)

    # å¯åŠ¨åå°çº¿ç¨‹å¤„ç†ä»»åŠ¡
    thread = threading.Thread(
        target=process_analysis_task,
        args=(task_id, url, file_data, session_id, user_id, username, department)
    )
    thread.daemon = True
    thread.start()

    logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²åˆ›å»ºå¹¶å¯åŠ¨")

    # ç«‹å³è¿”å›ä»»åŠ¡ ID
    return jsonify({
        'task_id': task_id,
        'status': 'pending',
        'message': 'ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°å¤„ç†...'
    })


@app.route('/task_status/<task_id>')
def task_status(task_id):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    task = get_task(task_id)

    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

    return jsonify({
        'status': task['status'],
        'progress': task['progress'],
        'result': task['result'],
        'error': task['error']
    })

# ============================================
# åŠŸèƒ½ 2: ç«å“ç›‘æ§
# ============================================

@app.route('/competitor-tool')
@login_required
def competitor_tool():
    """ç«å“ç›‘æ§å·¥å…·é¡µé¢"""
    return render_template('competitor.html')


@app.route('/monitor_competitors', methods=['POST'])
def monitor_competitors():
    """ç«å“ç›‘æ§ API"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ”¶åˆ°ç«å“ç›‘æ§è¯·æ±‚")

    try:
        data = request.json
        target_url = data.get('competitor_name')
        start_dt_str = data.get('startDate')
        end_dt_str = data.get('endDate')

        logger.info(f"ğŸ¯ ç›®æ ‡ URL: {target_url}")
        logger.info(f"ğŸ“… æ—¶é—´æ®µ: {start_dt_str} ~ {end_dt_str}")

        if not apify_client:
            error_msg = "âŒ é”™è¯¯ï¼šAPIFY_TOKEN æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨çˆ¬è™«åŠŸèƒ½"
            print(error_msg)
            return jsonify({'result': f"<div class='alert alert-danger'>{error_msg}</div>"})

        # 1. æ—¥æœŸè½¬æ¢
        target_start = datetime.datetime.strptime(start_dt_str, '%Y-%m-%d').date()
        target_end = datetime.datetime.strptime(end_dt_str, '%Y-%m-%d').date()
        logger.info(f"ğŸ“† è§£ææ—¥æœŸ: {target_start} ~ {target_end}")

        # 2. äº‘ç«¯æŠ“å–
        logger.info("ğŸ•µï¸ å¯åŠ¨ TikTok çˆ¬è™«...")
        run_input = {
            "profiles": [target_url],
            "resultsPerPage": 35,
            "oldestPostDate": start_dt_str,
            "shouldDownloadVideos": False
        }

        # ä½¿ç”¨ start() å¯åŠ¨çˆ¬è™«
        run = apify_client.actor("clockworks/tiktok-scraper").start(run_input=run_input)
        logger.info(f"âœ… çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼ŒRun ID: {run['id']}")

        # ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆæ­£ç¡®çš„å‚æ•°åï¼‰
        logger.info("â³ ç­‰å¾…çˆ¬è™«å®Œæˆ...")
        run = apify_client.run(run['id']).wait_for_finish(wait_secs=180)
        logger.info(f"âœ… çˆ¬è™«ä»»åŠ¡å®Œæˆï¼ŒçŠ¶æ€: {run['status']}")

        if run['status'] != 'SUCCEEDED':
            error_msg = f"âŒ çˆ¬è™«ä»»åŠ¡å¤±è´¥ï¼ŒçŠ¶æ€: {run['status']}"
            logger.error(error_msg)
            return jsonify({'result': f"<div class='alert alert-danger'>{error_msg}</div>"})

        items = apify_client.dataset(run["defaultDatasetId"]).list_items().items
        logger.info(f"ğŸ“¦ è·å–åˆ° {len(items)} æ¡åŸå§‹æ•°æ®")

        # 3. æœ¬åœ°æ—¶é—´è¿‡æ»¤
        cleaned = []
        for it in items:
            raw_date = it.get("createTimeISO")
            if not raw_date:
                continue

            post_dt = datetime.datetime.fromisoformat(raw_date.replace('Z', '+00:00')).date()

            if target_start <= post_dt <= target_end:
                cleaned.append({
                    "desc": it.get("desc", "æ— æè¿°"),
                    "likes": it.get("diggCount", 0),
                    "views": it.get("playCount", 0),
                    "comments": it.get("commentCount", 0),
                    "shares": it.get("shareCount", 0),
                    "collects": it.get("collectCount", 0),
                    "url": it.get("webVideoUrl"),
                    "date": str(post_dt)
                })

        logger.info(f"âœ… æ—¶é—´è¿‡æ»¤åå‰©ä½™ {len(cleaned)} æ¡æ•°æ®")

        if not cleaned:
            warning_msg = f"<div class='alert alert-warning'>åœ¨æ­¤æœŸé—´ ({start_dt_str} ~ {end_dt_str}) æœªå‘ç°è§†é¢‘ã€‚</div>"
            logger.info("âš ï¸ æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘")
            return jsonify({'result': warning_msg})

        # 4. Gemini ç”ŸæˆæŠ¥å‘Š
        prompt = f"""
You are a Data Entry Assistant. Please fill the following TikTok data into the PROVIDED HTML TEMPLATE.

ã€Data Sourceã€‘: {cleaned}
ã€Periodã€‘: {start_dt_str} to {end_dt_str}

ã€STRICT TEMPLATE (Use this EXACT structure)ã€‘:
<div style="width:100%; font-family:sans-serif;">
    <h3 style="color:#D32F2F; border-bottom:2px solid #eee; padding-bottom:10px;">ğŸ“Š æ•°æ®æ¦‚è§ˆè¡¨ ({start_dt_str} è‡³ {end_dt_str})</h3>
    <table class="table" style="width:100%; margin-bottom:30px; text-align:center; font-size:0.9rem;">
        <tr style="background:#f8f9fa;">
            <th>æ€»æ’­æ”¾</th><th>æ€»äº’åŠ¨</th><th>æ€»ç‚¹èµ</th><th>æ€»è¯„è®º</th><th>æ€»æ”¶è—</th><th>æ€»è½¬å‘</th>
        </tr>
        <tr>
            <td>[æ€»æ’­æ”¾æ•°]</td><td>[æ€»äº’åŠ¨æ•°]</td><td>[æ€»ç‚¹èµæ•°]</td><td>[æ€»è¯„è®ºæ•°]</td><td>[æ€»æ”¶è—æ•°]</td><td>[æ€»è½¬å‘æ•°]</td>
        </tr>
    </table>

    <h3 style="color:#D32F2F; border-bottom:2px solid #eee; padding-bottom:10px;">ğŸ”¥ çˆ†æ¬¾è§†é¢‘ç²¾é€‰</h3>
    <div style="background:#FFF9F9; border-left:5px solid #D32F2F; padding:20px; margin-bottom:15px; border-radius:8px;">
        <p><strong>è§†é¢‘æè¿°ï¼š</strong> [æè¿°å†…å®¹]</p>
        <p><strong>æ ¸å¿ƒæŒ‡æ ‡ï¼š</strong> æ’­æ”¾: [æ’­æ”¾æ•°] | ç‚¹èµ: [ç‚¹èµæ•°] | äº’åŠ¨: [è¯„è®ºæ•°]è¯„è®º / [åˆ†äº«æ•°]åˆ†äº«</p>
        <p><strong>æŸ¥çœ‹è¯¦æƒ…ï¼š</strong> <a href="[webVideoUrl]" target="_blank" style="color:#2962FF;">ç‚¹å‡»è¿›å…¥ TikTok è§‚çœ‹åŸæ–‡é“¾æ¥</a></p>
    </div>
</div>

ã€Requirementsã€‘:
- å¿…é¡»ä½¿ç”¨ä¸­æ–‡å¡«å……æ¨¡æ¿ã€‚
- æ€»äº’åŠ¨ = ç‚¹èµ + è¯„è®º + æ”¶è— + è½¬å‘çš„æ€»å’Œã€‚
- ä¸¥ç¦æ·»åŠ æ¨¡æ¿ä¹‹å¤–çš„ä»»ä½•æ–‡å­—ï¼ˆåŒ…æ‹¬åˆ†æã€å»ºè®®ã€å‰è¨€ã€ç»“è¯­ï¼‰ã€‚
- ä»…è¾“å‡º Raw HTML ä»£ç ï¼Œç¦æ­¢ Markdown ä»£ç å—ã€‚
"""

        logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨ Gemini API ç”ŸæˆæŠ¥å‘Š...")
        result, tokens = call_gemini(prompt)

        # æ¸…ç† Markdown ä»£ç å—æ ‡è®°
        result = result.replace('```html', '').replace('```', '').strip()

        # ä¿å­˜å†å²è®°å½•
        save_history(f"ç«å“æ•°æ®:{target_url[20:30]}", result, 'competitor')

        # è®°å½•ä½¿ç”¨æˆæœ¬
        if session.get('user_id'):
            log_usage(
                session.get('user_id'),
                session.get('username', 'unknown'),
                session.get('department', 'æœªçŸ¥'),
                'competitor',
                len(cleaned),  # TikTok è§†é¢‘æ•°é‡
                tokens
            )

        logger.info("âœ… ç«å“ç›‘æ§å®Œæˆ")
        logger.info("=" * 60 + "\n")

        return jsonify({'result': result})

    except Exception as e:
        error_msg = f"âŒ ç›‘æ§å¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return jsonify({'result': f"<div class='alert alert-danger'>{error_msg}</div>"})

# ============================================
# åŠŸèƒ½ 3: è§†é¢‘ç”Ÿæˆ
# ============================================

@app.route('/video-tool')
@login_required
def video_tool():
    """è§†é¢‘ç”Ÿæˆå·¥å…·é¡µé¢"""
    return render_template('video.html')


@app.route('/generate_video', methods=['POST'])
def generate_video():
    """è§†é¢‘ç”Ÿæˆ API"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ”¶åˆ°è§†é¢‘ç”Ÿæˆè¯·æ±‚")

    try:
        prompt = request.json.get('prompt')
        logger.info(f"ğŸ¬ Prompt: {prompt[:50]}...")

        video_url = call_veo_api(prompt)
        save_history(f"è§†é¢‘: {prompt[:10]}", video_url, 'video')

        logger.info("âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ")
        logger.info("=" * 60 + "\n")

        return jsonify({'video_url': video_url})

    except Exception as e:
        error_msg = f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return jsonify({'error': error_msg})

# ============================================
# å†å²è®°å½•ç®¡ç†
# ============================================

@app.route('/get_history')
@login_required
def get_history():
    """è·å–å†å²è®°å½•ï¼ˆä»æ•°æ®åº“ï¼‰"""
    try:
        user_id = session.get('user_id')

        # ä»æ•°æ®åº“è¯»å–ç”¨æˆ·çš„å†å²è®°å½•
        records = db.query_all("""
            SELECT id, title, result, type, created_at
            FROM analysis_results
            WHERE user_id = %s
            ORDER BY created_at DESC
            LIMIT 50
        """, (user_id,))

        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        result = []
        for record in records:
            result.append({
                'id': record['id'],
                'title': f"{record['title']} [{record['created_at'].strftime('%H:%M')}]",
                'result': record['result'],
                'type': record['type']
            })

        return jsonify(result)

    except Exception as e:
        logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è¿”å›å†…å­˜ä¸­çš„è®°å½•
        return jsonify(HISTORY_DB[::-1])


@app.route('/get_record/<int:id>')
@login_required
def get_record(id):
    """è·å–å•æ¡è®°å½•ï¼ˆä»æ•°æ®åº“ï¼‰"""
    try:
        user_id = session.get('user_id')

        # ä»æ•°æ®åº“è¯»å–ï¼ˆç¡®ä¿åªèƒ½è¯»å–è‡ªå·±çš„è®°å½•ï¼‰
        record = db.query_one("""
            SELECT id, title, result, type, created_at
            FROM analysis_results
            WHERE id = %s AND user_id = %s
        """, (id, user_id))

        if record:
            return jsonify({
                'id': record['id'],
                'title': record['title'],
                'result': record['result'],
                'type': record['type']
            })
        else:
            return jsonify({'error': 'è®°å½•ä¸å­˜åœ¨'}), 404

    except Exception as e:
        logger.error(f"âŒ è·å–è®°å½•å¤±è´¥: {e}")
        # å¤±è´¥æ—¶ä»å†…å­˜æŸ¥æ‰¾
        record = next((x for x in HISTORY_DB if x['id'] == id), None)
        return jsonify(record) if record else jsonify({'error': 'è®°å½•ä¸å­˜åœ¨'}), 404

# ============================================
# Excel å¯¼å‡ºåŠŸèƒ½
# ============================================

def create_excel_by_language(results):
    """æŒ‰è¯­è¨€åˆ†ç±»ç”Ÿæˆ Excel"""
    wb = Workbook()
    wb.remove(wb.active)  # åˆ é™¤é»˜è®¤ sheet

    # æŒ‰è¯­è¨€åˆ†ç»„
    language_groups = {}
    for item in results:
        lang = item.get('language', 'å…¶ä»–')
        if lang not in language_groups:
            language_groups[lang] = []
        language_groups[lang].append(item)

    # ä¸ºæ¯ä¸ªè¯­è¨€åˆ›å»º Sheet
    for lang, items in sorted(language_groups.items()):
        ws = wb.create_sheet(title=lang)

        # è¡¨å¤´
        headers = ['åºå·', 'åŸå§‹è¯„è®º', 'å½’ç±»', 'æƒ…æ„Ÿå€¾å‘', 'è¯­è¨€', 'ç®€è¦åˆ†æ']
        ws.append(headers)

        # è®¾ç½®è¡¨å¤´æ ·å¼
        header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
        header_font = Font(bold=True, color='FFFFFF')
        for cell in ws[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center')

        # æ·»åŠ æ•°æ®
        for idx, item in enumerate(items, 1):
            ws.append([
                idx,
                item.get('text', ''),
                item.get('category', ''),
                item.get('sentiment', ''),
                item.get('language', ''),
                item.get('analysis', '')
            ])

        # è®¾ç½®åˆ—å®½
        ws.column_dimensions['A'].width = 8
        ws.column_dimensions['B'].width = 50
        ws.column_dimensions['C'].width = 20
        ws.column_dimensions['D'].width = 12
        ws.column_dimensions['E'].width = 10
        ws.column_dimensions['F'].width = 40

        # å†»ç»“é¦–è¡Œ
        ws.freeze_panes = 'A2'

    return wb


def create_excel_by_category(results):
    """æŒ‰åˆ†ç±»ç”Ÿæˆ Excel"""
    wb = Workbook()
    wb.remove(wb.active)

    # æŒ‰åˆ†ç±»åˆ†ç»„
    category_groups = {}
    for item in results:
        cat = item.get('category', 'å…¶ä»–')
        if cat not in category_groups:
            category_groups[cat] = []
        category_groups[cat].append(item)

    # åˆ†ç±»é¡ºåºå’Œ Sheet åç§°æ˜ å°„ï¼ˆå»æ‰ç‰¹æ®Šå­—ç¬¦ï¼‰
    category_mapping = {
        'å¤–æŒ‚ä½œå¼Š': 'å¤–æŒ‚ä½œå¼Š',
        'æ¸¸æˆä¼˜åŒ–': 'æ¸¸æˆä¼˜åŒ–',
        'æ¸¸æˆBug': 'æ¸¸æˆBug',
        'å……å€¼é€€æ¬¾': 'å……å€¼é€€æ¬¾',
        'æ–°æ¨¡å¼/åœ°å›¾/å¹³è¡¡æ€§å»ºè®®': 'æ–°æ¨¡å¼åœ°å›¾å¹³è¡¡æ€§å»ºè®®'  # å»æ‰æ–œæ 
    }

    # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»º Sheet
    for cat, sheet_name in category_mapping.items():
        if cat not in category_groups:
            continue

        items = category_groups[cat]
        ws = wb.create_sheet(title=sheet_name)

        # è¡¨å¤´
        headers = ['åºå·', 'åŸå§‹è¯„è®º', 'å½’ç±»', 'æƒ…æ„Ÿå€¾å‘', 'è¯­è¨€', 'ç®€è¦åˆ†æ']
        ws.append(headers)

        # è®¾ç½®è¡¨å¤´æ ·å¼
        header_fill = PatternFill(start_color='D32F2F', end_color='D32F2F', fill_type='solid')
        header_font = Font(bold=True, color='FFFFFF')
        for cell in ws[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center')

        # æ·»åŠ æ•°æ®
        for idx, item in enumerate(items, 1):
            ws.append([
                idx,
                item.get('text', ''),
                item.get('category', ''),
                item.get('sentiment', ''),
                item.get('language', ''),
                item.get('analysis', '')
            ])

        # è®¾ç½®åˆ—å®½
        ws.column_dimensions['A'].width = 8
        ws.column_dimensions['B'].width = 50
        ws.column_dimensions['C'].width = 20
        ws.column_dimensions['D'].width = 12
        ws.column_dimensions['E'].width = 10
        ws.column_dimensions['F'].width = 40

        # å†»ç»“é¦–è¡Œ
        ws.freeze_panes = 'A2'

    return wb


@app.route('/export_by_language')
@login_required
def export_by_language():
    """æŒ‰è¯­è¨€å¯¼å‡º Excel"""
    session_id = session.get('session_id', 'default')
    results = LATEST_ANALYSIS_RESULTS.get(session_id, [])

    if not results:
        return jsonify({'error': 'æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®'}), 400

    try:
        wb = create_excel_by_language(results)

        # ç”Ÿæˆæ–‡ä»¶
        output = BytesIO()
        wb.save(output)
        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
        filename = f'è¯­è¨€åˆ†ç±»æŠ¥å‘Š_{timestamp}.xlsx'

        logger.info(f"ğŸ“¥ å¯¼å‡ºè¯­è¨€åˆ†ç±»æŠ¥å‘Š: {len(results)} æ¡æ•°æ®")

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=filename
        )
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/export_by_category')
@login_required
def export_by_category():
    """æŒ‰åˆ†ç±»å¯¼å‡º Excel"""
    session_id = session.get('session_id', 'default')
    results = LATEST_ANALYSIS_RESULTS.get(session_id, [])

    if not results:
        return jsonify({'error': 'æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®'}), 400

    try:
        wb = create_excel_by_category(results)

        # ç”Ÿæˆæ–‡ä»¶
        output = BytesIO()
        wb.save(output)
        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
        filename = f'é—®é¢˜åˆ†ç±»æŠ¥å‘Š_{timestamp}.xlsx'

        logger.info(f"ğŸ“¥ å¯¼å‡ºé—®é¢˜åˆ†ç±»æŠ¥å‘Š: {len(results)} æ¡æ•°æ®")

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=filename
        )
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# ============================================
# ç»Ÿè®¡åŠŸèƒ½
# ============================================

@app.route('/my-stats')
@login_required
def my_stats():
    """ä¸ªäººç»Ÿè®¡é¡µé¢"""
    user_id = session.get('user_id')

    # è·å–æœ¬æœˆç»Ÿè®¡æ•°æ®
    current_month = datetime.datetime.now().strftime('%Y-%m')

    stats_data = db.query_one("""
        SELECT
            COUNT(*) as count,
            COALESCE(SUM(comments_count), 0) as comments,
            COALESCE(SUM(total_cost), 0) as cost
        FROM usage_logs
        WHERE user_id = %s
          AND TO_CHAR(created_at, 'YYYY-MM') = %s
    """, (user_id, current_month))

    stats = {
        'count': stats_data['count'] if stats_data else 0,
        'comments': stats_data['comments'] if stats_data else 0,
        'cost': float(stats_data['cost']) if stats_data else 0.0,
        'avg_cost': float(stats_data['cost']) / stats_data['count'] if stats_data and stats_data['count'] > 0 else 0.0
    }

    # è·å–æœ€è¿‘20æ¡ä½¿ç”¨è®°å½•
    logs = db.query_all("""
        SELECT *
        FROM usage_logs
        WHERE user_id = %s
        ORDER BY created_at DESC
        LIMIT 20
    """, (user_id,))

    return render_template('my_stats.html', stats=stats, logs=logs, user=session)


@app.route('/admin')
@admin_required
def admin_panel():
    """ç®¡ç†åå°é¡µé¢"""
    current_month = datetime.datetime.now().strftime('%Y-%m')

    # å…¨å±€ç»Ÿè®¡
    global_data = db.query_one("""
        SELECT
            COALESCE(SUM(total_cost), 0) as total_cost,
            COUNT(DISTINCT user_id) as active_users,
            COUNT(*) as total_count
        FROM usage_logs
        WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
    """, (current_month,))

    global_stats = {
        'total_cost': float(global_data['total_cost']) if global_data else 0.0,
        'active_users': global_data['active_users'] if global_data else 0,
        'total_count': global_data['total_count'] if global_data else 0,
        'avg_cost': float(global_data['total_cost']) / global_data['total_count'] if global_data and global_data['total_count'] > 0 else 0.0
    }

    # éƒ¨é—¨ç»Ÿè®¡
    dept_stats = db.query_all("""
        SELECT
            department,
            COALESCE(SUM(total_cost), 0) as cost,
            COUNT(*) as count
        FROM usage_logs
        WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
        GROUP BY department
        ORDER BY cost DESC
    """, (current_month,))

    # ç”¨æˆ·ç»Ÿè®¡ï¼ˆTop 10ï¼‰
    user_stats = db.query_all("""
        SELECT
            u.real_name,
            u.department,
            COALESCE(SUM(l.total_cost), 0) as cost,
            COUNT(l.id) as count
        FROM users u
        LEFT JOIN usage_logs l ON u.id = l.user_id
            AND TO_CHAR(l.created_at, 'YYYY-MM') = %s
        GROUP BY u.id, u.real_name, u.department
        ORDER BY cost DESC
        LIMIT 10
    """, (current_month,))

    # æ‰€æœ‰ç”¨æˆ·åˆ—è¡¨
    all_users = db.query_all("""
        SELECT * FROM users ORDER BY created_at DESC
    """)

    return render_template('admin.html',
                         global_stats=global_stats,
                         dept_stats=dept_stats,
                         user_stats=user_stats,
                         all_users=all_users,
                         user=session)


@app.route('/admin/add_user', methods=['POST'])
@admin_required
def add_user():
    """æ·»åŠ æ–°ç”¨æˆ·"""
    try:
        data = request.json
        username = data.get('username')
        password = data.get('password')
        real_name = data.get('real_name')
        department = data.get('department')
        role = data.get('role', 'user')

        # æ£€æŸ¥ç”¨æˆ·åæ˜¯å¦å·²å­˜åœ¨
        existing = db.query_one("SELECT id FROM users WHERE username = %s", (username,))
        if existing:
            return jsonify({'error': 'ç”¨æˆ·åå·²å­˜åœ¨'}), 400

        # åŠ å¯†å¯†ç 
        password_hash = bcrypt.generate_password_hash(password).decode('utf-8')

        # æ’å…¥ç”¨æˆ·
        db.execute("""
            INSERT INTO users (username, password_hash, real_name, department, role)
            VALUES (%s, %s, %s, %s, %s)
        """, (username, password_hash, real_name, department, role))

        logger.info(f"âœ… ç®¡ç†å‘˜æ·»åŠ æ–°ç”¨æˆ·: {username} ({real_name})")

        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"âŒ æ·»åŠ ç”¨æˆ·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/admin/delete_user/<int:user_id>', methods=['DELETE'])
@admin_required
def delete_user(user_id):
    """åˆ é™¤ç”¨æˆ·"""
    try:
        # ä¸å…è®¸åˆ é™¤ç®¡ç†å‘˜è´¦å·
        user = db.query_one("SELECT username FROM users WHERE id = %s", (user_id,))
        if user and user['username'] == 'admin':
            return jsonify({'error': 'ä¸èƒ½åˆ é™¤ç®¡ç†å‘˜è´¦å·'}), 403

        # åˆ é™¤ç”¨æˆ·
        db.execute("DELETE FROM users WHERE id = %s", (user_id,))

        logger.info(f"âœ… ç®¡ç†å‘˜åˆ é™¤ç”¨æˆ·: ID={user_id}")

        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ç”¨æˆ·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


# ============================================
# åº”ç”¨å¯åŠ¨
# ============================================

if __name__ == '__main__':
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ Sailson AI å·¥ä½œå°å·²å¯åŠ¨")
    logger.info(f"ğŸŒ è®¿é—®åœ°å€: http://0.0.0.0:{PORT}")
    logger.info("=" * 60 + "\n")

    app.run(debug=False, host='0.0.0.0', port=PORT)
