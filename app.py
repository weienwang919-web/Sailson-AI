import os
import sys
import datetime
import time
import logging
import smtplib
import json
from email.mime.text import MIMEText
import pandas as pd
import uuid
import threading
import requests
from io import BytesIO
from PIL import Image
from flask import Flask, render_template, request, jsonify, redirect, url_for, session, send_file
from flask_bcrypt import Bcrypt
from apify_client import ApifyClient
from dotenv import load_dotenv
from openai import OpenAI
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from functools import wraps
import html
import database as db

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ============================================
# æ—¥å¿—é…ç½® - ç¡®ä¿è¾“å‡ºåˆ° stdout
# ============================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ============================================
# ç¯å¢ƒé…ç½® - ä¼˜åŒ–ç‰ˆ
# ============================================

# æ¸…é™¤ä»£ç†è®¾ç½®ï¼ˆäº‘ç«¯ç¯å¢ƒä¸éœ€è¦ä»£ç†ï¼‰
for proxy_var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    if proxy_var in os.environ:
        del os.environ[proxy_var]
        logger.info(f"ğŸ§¹ å·²æ¸…é™¤ä»£ç†è®¾ç½®: {proxy_var}")

# åŠ è½½ç¯å¢ƒå˜é‡
DASHSCOPE_API_KEY = os.environ.get('DASHSCOPE_API_KEY')
APIFY_TOKEN = os.environ.get('APIFY_TOKEN')
PORT = int(os.environ.get('PORT', 5001))

# é•¿ä»»åŠ¡å¤„ç†æ¨¡å¼é…ç½®ï¼ˆé¢„ç•™å¼€å…³ï¼Œé»˜è®¤ä¿æŒç°çŠ¶ï¼šç”± Web çº¿ç¨‹æ‰§è¡Œï¼‰
USE_DB_WORKER = os.environ.get('USE_DB_WORKER', 'false').lower() == 'true'

# åé¦ˆé‚®ä»¶é…ç½®ï¼ˆå¯é€‰ï¼‰
SMTP_HOST = os.environ.get('SMTP_HOST')
SMTP_PORT = int(os.environ.get('SMTP_PORT', '587'))
SMTP_USER = os.environ.get('SMTP_USER')
SMTP_PASS = os.environ.get('SMTP_PASS')
FEEDBACK_EMAIL_TO = os.environ.get('FEEDBACK_EMAIL_TO')
FEEDBACK_EMAIL_FROM = os.environ.get('FEEDBACK_EMAIL_FROM', SMTP_USER or FEEDBACK_EMAIL_TO or '')

# å¯åŠ¨æ—¶è¾“å‡ºé…ç½®çŠ¶æ€
logger.info("=" * 60)
logger.info("ğŸš€ Sailson AI å·¥ä½œå°å¯åŠ¨ä¸­...")
logger.info(f"ğŸ”‘ DASHSCOPE_API_KEY: {'âœ… å·²é…ç½®' if DASHSCOPE_API_KEY else 'âŒ æœªé…ç½®'}")
logger.info(f"ğŸ”‘ APIFY_TOKEN: {'âœ… å·²é…ç½®' if APIFY_TOKEN else 'âŒ æœªé…ç½®'}")
logger.info(f"ğŸŒ PORT: {PORT}")
logger.info(f"ğŸ§µ Long-task mode: {'DB worker' if USE_DB_WORKER else 'in-process threads'}")
logger.info(f"ğŸ Python ç‰ˆæœ¬: {sys.version}")
logger.info("=" * 60)

# åˆå§‹åŒ– AI å¼•æ“
if DASHSCOPE_API_KEY:
    try:
        qwen_client = OpenAI(
            api_key=DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        logger.info("âœ… é€šä¹‰åƒé—® API åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ é€šä¹‰åƒé—® API åˆå§‹åŒ–å¤±è´¥: {e}")
        qwen_client = None
else:
    logger.warning("âš ï¸ è­¦å‘Š: DASHSCOPE_API_KEY æœªé…ç½®ï¼ŒAI åŠŸèƒ½å°†ä¸å¯ç”¨")
    qwen_client = None

# ä¸å†åˆå§‹åŒ–å…¨å±€ Apify å®¢æˆ·ç«¯ï¼Œæ”¹ç”¨ REST API
# åªæ£€æŸ¥ token æ˜¯å¦å­˜åœ¨
if APIFY_TOKEN:
    logger.info("âœ… APIFY_TOKEN å·²é…ç½®")
else:
    logger.warning("âš ï¸ è­¦å‘Š: APIFY_TOKEN æœªé…ç½®ï¼Œçˆ¬è™«åŠŸèƒ½å°†ä¸å¯ç”¨")

# Flask åº”ç”¨åˆå§‹åŒ–
app = Flask(__name__)

secret_key = os.environ.get('SECRET_KEY')
if not secret_key:
    # ä¸ºäº†ä¸å½±å“ç°æœ‰åŠŸèƒ½ï¼Œåœ¨ç¼ºå°‘ SECRET_KEY æ—¶è‡ªåŠ¨ç”Ÿæˆä¸€æ¬¡æ€§å¼€å‘å¯†é’¥
    # ç”Ÿäº§ç¯å¢ƒå¿…é¡»é€šè¿‡ç¯å¢ƒå˜é‡æ˜¾å¼é…ç½® SECRET_KEY
    logger.warning("âš ï¸ SECRET_KEY æœªé…ç½®ï¼Œå°†ä½¿ç”¨ä¸´æ—¶å¼€å‘å¯†é’¥ã€‚è¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½® SECRET_KEYï¼")
    import secrets
    secret_key = "dev-" + secrets.token_hex(32)

app.secret_key = secret_key
bcrypt = Bcrypt(app)

# å†…å­˜å­˜å‚¨ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
HISTORY_DB = []
LATEST_ANALYSIS_RESULTS = {}  # å­˜å‚¨æœ€æ–°çš„åˆ†æç»“æœï¼Œç”¨äºå¯¼å‡º
# TASK_QUEUE å·²è¿ç§»åˆ°æ•°æ®åº“ï¼Œä¸å†ä½¿ç”¨å†…å­˜å­—å…¸

# task_queue è¡¨ç»“æ„çŠ¶æ€ï¼ˆç”¨äºå‘åå…¼å®¹è€æ•°æ®åº“ï¼‰
TASK_QUEUE_HAS_FUNCTION_TYPE = True
ANALYSIS_RESULTS_HAS_JSON = True

# é¡¹ç›®ä¸æç¤ºè¯å¤–ç½®ï¼ˆå¤šé¡¹ç›®æ¥å…¥ï¼‰
VALID_PROJECTS = ('CFL', 'PUBGM', 'HOK')
_PROMPTS_CACHE = None

def load_prompts():
    """åŠ è½½ config/prompts.jsonï¼›ç¼ºå¤±æ—¶å›é€€åˆ° CFL ç¡¬ç¼–ç ï¼Œä¿è¯ç°æœ‰è¡Œä¸ºä¸å˜ã€‚"""
    global _PROMPTS_CACHE
    if _PROMPTS_CACHE is not None:
        return _PROMPTS_CACHE
    fallback = {
        'sentiment': {
            'CFL': (
                "Analyze these comments and categorize them. Output ONLY a JSON array.\n\n"
                "Comments:\n{batch_content}\n\n"
                "Categories (Chinese only):\n1. å¤–æŒ‚ä½œå¼Š - hackers, cheating\n2. æ¸¸æˆä¼˜åŒ– - lag, crashes\n"
                "3. æ¸¸æˆBug - glitches, errors\n4. å……å€¼é€€æ¬¾ - payment issues\n"
                "5. æ–°æ¨¡å¼/åœ°å›¾/å¹³è¡¡æ€§å»ºè®® - new content requests\n6. å…¶ä»– - spam, praise\n\n"
                "Output format (JSON array only, no markdown):\n[\n  {{\n    \"text\": \"comment text\",\n    \"category\": \"å¤–æŒ‚ä½œå¼Š\",\n"
                "    \"sentiment\": \"è´Ÿé¢\",\n    \"language\": \"è‹±è¯­\",\n    \"analysis\": \"è¯¦ç»†åˆ†æå†…å®¹\"\n  }},\n  ...\n]\n\n"
                "IMPORTANT:\n- Output ONLY valid JSON array\n"
                "- Output exactly one object per comment; do NOT skip any comment (use category \"å…¶ä»–\" for spam/praise if needed)\n"
                "- Use Chinese for category, sentiment, language, and analysis\n"
                "- Language options (MUST be one of these): è‹±è¯­, è²å¾‹å®¾è¯­, æ³°è¯­, è¶Šå—è¯­, å°å°¼è¯­, é©¬æ¥è¯­\n"
                "- Identify the language accurately based on the text\n"
                "- æœ¬æŠ¥å‘Šä¾› CFL å“ç‰Œæ–¹/å®¢æˆ·æŸ¥çœ‹ï¼Œç®€è¦åˆ†æéœ€ä¾¿äºå¿«é€ŸæŠŠæ¡ç©å®¶è¯‰æ±‚ä¸æƒ…ç»ªã€‚\n"
                "- Analysisï¼ˆç®€è¦åˆ†æï¼‰å­—æ•°è¦æ±‚ï¼Œå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼š\n"
                "  * çŸ­è¯„è®ºï¼ˆåŸæ–‡ < 30 å­—ï¼‰ï¼šä¸€å¥è¯æ¦‚æ‹¬ï¼Œ15-20 ä¸ªä¸­æ–‡å­—ã€‚\n"
                "  * ä¸­ç­‰è¯„è®ºï¼ˆ30-80 å­—ï¼‰ï¼šä¸¤è‡³ä¸‰å¥è¯ï¼ŒåŒ…å«é—®é¢˜ç‚¹ä¸æƒ…ç»ªï¼Œ50-70 ä¸ªä¸­æ–‡å­—ã€‚\n"
                "  * é•¿è¯„è®ºï¼ˆâ‰¥ 80 å­—ï¼‰ï¼šå±•å¼€åˆ†æï¼ŒåŒ…å«ä¸»è¦è¯‰æ±‚ã€ç©å®¶æƒ…ç»ªã€å…³é”®ç»†èŠ‚åŠå¯¹å“ç‰Œçš„å‚è€ƒç‚¹ï¼Œ80-120 ä¸ªä¸­æ–‡å­—ï¼Œä¸å¾—ä»…ç”¨ä¸€å¥è¯æ€»ç»“ã€‚\n"
                "  * å†…å®¹é¡»åŒ…å«ï¼šä¸»è¦é—®é¢˜ã€ç©å®¶æƒ…ç»ªã€å…³é”®ç»†èŠ‚ï¼›é•¿è¯„è®ºåŠ¡å¿…å¤šå¥å±•å¼€ï¼Œä¸å¯å…¨éƒ¨ç»Ÿä¸€ä¸ºä¸€å¥è¯æ€»ç»“ã€‚\n"
            ),
            'PUBGM': '',
            'HOK': '',
        },
        'competitor': {
            'CFL': (
                "You are a Data Entry Assistant. Please fill the following TikTok data into the PROVIDED HTML TEMPLATE.\n\n"
                "ã€Data Sourceã€‘: {cleaned}\nã€Periodã€‘: {start_dt_str} to {end_dt_str}\n\n"
                "ã€STRICT TEMPLATE (Use this EXACT structure)ã€‘:\n<div style=\"width:100%; font-family:sans-serif;\">\n"
                "    <h3 style=\"color:#D32F2F; border-bottom:2px solid #eee; padding-bottom:10px;\">ğŸ“Š æ•°æ®æ¦‚è§ˆè¡¨ ({start_dt_str} è‡³ {end_dt_str})</h3>\n"
                "    <table class=\"table\" style=\"width:100%; margin-bottom:30px; text-align:center; font-size:0.9rem;\">\n"
                "        <tr style=\"background:#f8f9fa;\">\n"
                "            <th>æ€»æ’­æ”¾</th><th>æ€»äº’åŠ¨</th><th>æ€»ç‚¹èµ</th><th>æ€»è¯„è®º</th><th>æ€»æ”¶è—</th><th>æ€»è½¬å‘</th>\n"
                "        </tr>\n        <tr>\n"
                "            <td>[æ€»æ’­æ”¾æ•°]</td><td>[æ€»äº’åŠ¨æ•°]</td><td>[æ€»ç‚¹èµæ•°]</td><td>[æ€»è¯„è®ºæ•°]</td><td>[æ€»æ”¶è—æ•°]</td><td>[æ€»è½¬å‘æ•°]</td>\n"
                "        </tr>\n    </table>\n\n"
                "    <h3 style=\"color:#D32F2F; border-bottom:2px solid #eee; padding-bottom:10px;\">ğŸ”¥ çˆ†æ¬¾è§†é¢‘ç²¾é€‰</h3>\n"
                "    <div style=\"background:#FFF9F9; border-left:5px solid #D32F2F; padding:20px; margin-bottom:15px; border-radius:8px;\">\n"
                "        <p><strong>è§†é¢‘æè¿°ï¼š</strong> [æè¿°å†…å®¹]</p>\n"
                "        <p><strong>æ ¸å¿ƒæŒ‡æ ‡ï¼š</strong> æ’­æ”¾: [æ’­æ”¾æ•°] | ç‚¹èµ: [ç‚¹èµæ•°] | äº’åŠ¨: [è¯„è®ºæ•°]è¯„è®º / [åˆ†äº«æ•°]åˆ†äº«</p>\n"
                "        <p><strong>æŸ¥çœ‹è¯¦æƒ…ï¼š</strong> <a href=\"[webVideoUrl]\" target=\"_blank\" style=\"color:#2962FF;\">ç‚¹å‡»è¿›å…¥ TikTok è§‚çœ‹åŸæ–‡é“¾æ¥</a></p>\n"
                "    </div>\n</div>\n\n"
                "ã€Requirementsã€‘:\n- å¿…é¡»ä½¿ç”¨ä¸­æ–‡å¡«å……æ¨¡æ¿ã€‚\n- æ€»äº’åŠ¨ = ç‚¹èµ + è¯„è®º + æ”¶è— + è½¬å‘çš„æ€»å’Œã€‚\n"
                "- ä¸¥ç¦æ·»åŠ æ¨¡æ¿ä¹‹å¤–çš„ä»»ä½•æ–‡å­—ï¼ˆåŒ…æ‹¬åˆ†æã€å»ºè®®ã€å‰è¨€ã€ç»“è¯­ï¼‰ã€‚\n- ä»…è¾“å‡º Raw HTML ä»£ç ï¼Œç¦æ­¢ Markdown ä»£ç å—ã€‚\n"
            ),
            'PUBGM': '',
            'HOK': '',
        },
    }
    config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config', 'prompts.json')
    try:
        if os.path.isfile(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            for feat in ('sentiment', 'competitor'):
                if feat not in data:
                    data[feat] = fallback[feat]
                else:
                    for proj in VALID_PROJECTS:
                        if proj not in data[feat]:
                            data[feat][proj] = fallback[feat].get(proj, '')
            _PROMPTS_CACHE = data
            logger.info("âœ… å·²åŠ è½½ config/prompts.json")
        else:
            _PROMPTS_CACHE = fallback
            logger.info("âš ï¸ config/prompts.json ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½® CFL æç¤ºè¯")
    except Exception as e:
        logger.warning(f"âš ï¸ åŠ è½½ config/prompts.json å¤±è´¥: {e}ï¼Œä½¿ç”¨å†…ç½® CFL æç¤ºè¯")
        _PROMPTS_CACHE = fallback
    return _PROMPTS_CACHE

def get_prompt(feature, project):
    """å–æŒ‡å®šåŠŸèƒ½ã€é¡¹ç›®çš„æç¤ºè¯æ¨¡æ¿ã€‚feature ä¸º 'sentiment' æˆ– 'competitor'ï¼Œproject ä¸º CFL/PUBGM/HOKã€‚"""
    if project not in VALID_PROJECTS:
        return ''
    prompts = load_prompts()
    by_feature = prompts.get(feature, {})
    return (by_feature.get(project) or '').strip()


def ensure_task_queue_schema():
    """ç¡®ä¿ task_queue è¡¨åŒ…å« function_type å­—æ®µï¼ˆå‘åå…¼å®¹è€ç‰ˆæœ¬æ•°æ®åº“ï¼‰

    - æ­£å¸¸æƒ…å†µä¸‹ä¼šæ‰§è¡Œä¸€æ¬¡ ALTER TABLE ADD COLUMN IF NOT EXISTS
    - è‹¥å½“å‰æ•°æ®åº“ç”¨æˆ·æ— æƒé™ï¼Œæˆ–è¡¨ä¸å­˜åœ¨ï¼Œåªè®°å½• warningï¼Œä¸ä¸­æ–­å¯åŠ¨
    - create_task ä¼šæ ¹æ® TASK_QUEUE_HAS_FUNCTION_TYPE è‡ªåŠ¨é™çº§ä¸ºè€çš„æ’å…¥æ–¹å¼
    """
    global TASK_QUEUE_HAS_FUNCTION_TYPE
    try:
        db.execute("""
            ALTER TABLE task_queue
            ADD COLUMN IF NOT EXISTS function_type VARCHAR(50)
        """)
        logger.info("âœ… å·²ç¡®è®¤ task_queue.function_type åˆ—å­˜åœ¨")
        TASK_QUEUE_HAS_FUNCTION_TYPE = True
    except Exception as e:
        TASK_QUEUE_HAS_FUNCTION_TYPE = False
        logger.warning(f"âš ï¸ æ— æ³•è‡ªåŠ¨ä¸º task_queue æ·»åŠ  function_type åˆ—ï¼Œå°†ä½¿ç”¨å…¼å®¹æ¨¡å¼: {e}")


def ensure_analysis_results_schema():
    """ç¡®ä¿ analysis_results è¡¨åŒ…å« result_json å­—æ®µï¼ˆç”¨äºå¯¼å‡ºç»“æ„åŒ–ç»“æœï¼‰

    - æ­£å¸¸æƒ…å†µä¸‹ä¼šæ‰§è¡Œä¸€æ¬¡ ALTER TABLE ADD COLUMN IF NOT EXISTS
    - è‹¥å½“å‰æ•°æ®åº“ç”¨æˆ·æ— æƒé™ï¼Œæˆ–è¡¨ä¸å­˜åœ¨ï¼Œåªè®°å½• warningï¼Œä¸ä¸­æ–­å¯åŠ¨
    """
    global ANALYSIS_RESULTS_HAS_JSON
    try:
        db.execute("""
            ALTER TABLE analysis_results
            ADD COLUMN IF NOT EXISTS result_json TEXT
        """)
        logger.info("âœ… å·²ç¡®è®¤ analysis_results.result_json åˆ—å­˜åœ¨")
        ANALYSIS_RESULTS_HAS_JSON = True
    except Exception as e:
        ANALYSIS_RESULTS_HAS_JSON = False
        logger.warning(f"âš ï¸ æ— æ³•è‡ªåŠ¨ä¸º analysis_results æ·»åŠ  result_json åˆ—ï¼Œå°†æš‚ä¸æ”¯æŒæŒ‰ä»»æ„å†å²è®°å½•å¯¼å‡º: {e}")

def send_feedback_email(project_name: str, feedback: str) -> bool:
    """å‘é€ç”¨æˆ·åé¦ˆé‚®ä»¶åˆ°è¿ç»´é‚®ç®±ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

    ä¾èµ–ç¯å¢ƒå˜é‡ï¼š
    - SMTP_HOST / SMTP_PORT / SMTP_USER / SMTP_PASS
    - FEEDBACK_EMAIL_TO
    """
    if not (SMTP_HOST and SMTP_USER and SMTP_PASS and FEEDBACK_EMAIL_TO):
        logger.warning("âš ï¸ åé¦ˆé‚®ä»¶æœªå‘é€ï¼šSMTP æˆ–æ”¶ä»¶äººç¯å¢ƒå˜é‡æœªå®Œæ•´é…ç½®")
        return False

    try:
        subject = f"æ–°ç”¨æˆ·åé¦ˆ - {project_name}"
        body = f"é¡¹ç›®åç§°: {project_name}\n\nåé¦ˆå†…å®¹:\n{feedback}"

        msg = MIMEText(body, "plain", "utf-8")
        msg["Subject"] = subject
        msg["From"] = FEEDBACK_EMAIL_FROM or SMTP_USER
        msg["To"] = FEEDBACK_EMAIL_TO

        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASS)
            server.send_message(msg)

        logger.info("âœ… åé¦ˆé‚®ä»¶å‘é€æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"âŒ åé¦ˆé‚®ä»¶å‘é€å¤±è´¥: {e}")
        return False


# æ±‡ç‡é…ç½®
USD_TO_CNY = 7.2

# ============================================
# ä»»åŠ¡æ¢å¤æœºåˆ¶ï¼ˆå®šä¹‰ï¼Œç¨åè°ƒç”¨ï¼‰
# ============================================

# å¯åŠ¨æ—¶å°½æ—©æ£€æŸ¥ç›¸å…³è¡¨ç»“æ„
ensure_task_queue_schema()
ensure_analysis_results_schema()


def recover_interrupted_tasks():
    """æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡"""
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ processing çŠ¶æ€çš„ä»»åŠ¡ï¼ˆè¯´æ˜è¢«ä¸­æ–­äº†ï¼‰
        interrupted_tasks = db.query_all("""
            SELECT task_id FROM task_queue
            WHERE status = 'processing'
            AND created_at > NOW() - INTERVAL '1 hour'
        """)

        if interrupted_tasks:
            logger.warning(f"âš ï¸ å‘ç° {len(interrupted_tasks)} ä¸ªè¢«ä¸­æ–­çš„ä»»åŠ¡ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
            for task in interrupted_tasks:
                update_task(
                    task['task_id'],
                    status='failed',
                    error='æœåŠ¡é‡å¯å¯¼è‡´ä»»åŠ¡ä¸­æ–­',
                    progress='ä»»åŠ¡å·²ä¸­æ–­'
                )
    except Exception as e:
        logger.error(f"âŒ æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")

# ============================================
# è£…é¥°å™¨ï¼šæƒé™æ§åˆ¶
# ============================================

def login_required(f):
    """éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def admin_required(f):
    """éœ€è¦ç®¡ç†å‘˜æƒé™æ‰èƒ½è®¿é—®"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        if session.get('role') != 'admin':
            return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403
        return f(*args, **kwargs)
    return decorated_function

# ============================================
# æ ¸å¿ƒå·¥å…·å‡½æ•°
# ============================================

def create_task(task_id, user_id, session_id, function_type=None):
    """åˆ›å»ºä»»åŠ¡è®°å½•

    ä¸ºå…¼å®¹æ—§åº“ï¼š
    - ä¼˜å…ˆå°è¯•å†™å…¥ function_type å­—æ®µ
    - è‹¥å­—æ®µä¸å­˜åœ¨æˆ–æ— æƒé™ï¼Œåˆ™é€€å›è€çš„æ’å…¥æ–¹å¼
    """
    global TASK_QUEUE_HAS_FUNCTION_TYPE

    try:
        if TASK_QUEUE_HAS_FUNCTION_TYPE:
            db.execute("""
                INSERT INTO task_queue (task_id, user_id, session_id, function_type, status, progress)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (task_id, user_id, session_id, function_type, 'pending', 'ä»»åŠ¡å·²åˆ›å»º'))
        else:
            db.execute("""
                INSERT INTO task_queue (task_id, user_id, session_id, status, progress)
                VALUES (%s, %s, %s, %s, %s)
            """, (task_id, user_id, session_id, 'pending', 'ä»»åŠ¡å·²åˆ›å»º'))

        logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²å†™å…¥æ•°æ®åº“ï¼ˆtype={function_type}ï¼‰")
    except Exception as e:
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å†™å…¥å‘ç°æ²¡æœ‰ function_type åˆ—ï¼Œåˆ™è‡ªåŠ¨é™çº§ä¸ºæ—§æ¨¡å¼
        if TASK_QUEUE_HAS_FUNCTION_TYPE and 'function_type' in str(e):
            logger.warning(f"âš ï¸ task_queue ç¼ºå°‘ function_type åˆ—ï¼Œé™çº§ä¸ºå…¼å®¹æ¨¡å¼: {e}")
            TASK_QUEUE_HAS_FUNCTION_TYPE = False
            try:
                db.execute("""
                    INSERT INTO task_queue (task_id, user_id, session_id, status, progress)
                    VALUES (%s, %s, %s, %s, %s)
                """, (task_id, user_id, session_id, 'pending', 'ä»»åŠ¡å·²åˆ›å»º'))
                logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²åœ¨å…¼å®¹æ¨¡å¼ä¸‹å†™å…¥æ•°æ®åº“")
            except Exception as e2:
                logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰: {e2}")
        else:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")

def update_task(task_id, status=None, progress=None, result=None, error=None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    try:
        updates = []
        params = []

        if status is not None:
            updates.append("status = %s")
            params.append(status)
        if progress is not None:
            updates.append("progress = %s")
            params.append(progress)
        if result is not None:
            updates.append("result = %s")
            params.append(result)
        if error is not None:
            updates.append("error = %s")
            params.append(error)

        if updates:
            updates.append("updated_at = CURRENT_TIMESTAMP")
            params.append(task_id)

            sql = f"UPDATE task_queue SET {', '.join(updates)} WHERE task_id = %s"
            db.execute(sql, tuple(params))
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")

def get_task(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        task = db.query_one("""
            SELECT task_id, status, progress, result, error
            FROM task_queue
            WHERE task_id = %s
        """, (task_id,))
        return task
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return None

# ============================================
# å¯åŠ¨æ—¶æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡
# ============================================
recover_interrupted_tasks()


def call_gemini(prompt, image=None, timeout=60):
    """è°ƒç”¨é€šä¹‰åƒé—® API"""
    if not qwen_client:
        error_msg = "âŒ é”™è¯¯ï¼šDASHSCOPE_API_KEY æœªé…ç½®"
        logger.error(error_msg)
        return error_msg, 0

    model_name = 'qwen-turbo'

    try:
        logger.info(f"ğŸ¤– æ­£åœ¨è°ƒç”¨é€šä¹‰åƒé—®æ¨¡å‹: {model_name}")
        logger.info(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

        response = qwen_client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )

        result = response.choices[0].message.content
        tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0
        logger.info(f"âœ… é€šä¹‰åƒé—®è°ƒç”¨æˆåŠŸï¼Œè¿”å› {len(result)} å­—ç¬¦ï¼Œæ¶ˆè€— {tokens} tokens")
        return result, tokens

    except Exception as e:
        error_msg = f"âš ï¸ é€šä¹‰åƒé—® API è°ƒç”¨å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg, 0


def process_uploaded_file(file_data):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå›¾ç‰‡æˆ–è¡¨æ ¼ï¼‰

    Args:
        file_data: å­—å…¸ï¼ŒåŒ…å« filename, content, content_type
    """
    try:
        fname = file_data['filename'].lower()
        content = file_data['content']
        logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {fname}")

        if fname.endswith(('.png', '.jpg', '.jpeg', '.webp')):
            logger.info("ğŸ–¼ï¸ è¯†åˆ«ä¸ºå›¾ç‰‡æ–‡ä»¶")
            return "IMAGE", Image.open(BytesIO(content))

        if fname.endswith(('.xlsx', '.csv')):
            logger.info("ğŸ“Š è¯†åˆ«ä¸ºè¡¨æ ¼æ–‡ä»¶")
            if fname.endswith('.csv'):
                df = pd.read_csv(BytesIO(content))
            else:
                df = pd.read_excel(BytesIO(content))
            return "TEXT", df.to_string(index=False, max_rows=50)

        return "ERROR", "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"

    except Exception as e:
        error_msg = f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
        logger.info(f"âŒ {error_msg}")
        return "ERROR", error_msg


def save_history(user_id, title, result, type_tag, structured=None):
    """ä¿å­˜åˆ°å†å²è®°å½•ï¼ˆæ•°æ®åº“ + å†…å­˜ï¼‰ï¼Œå¹¶å¯é€‰ä¿å­˜ç»“æ„åŒ–ç»“æœ

    - user_id: ç”¨æˆ· IDï¼ˆå¯ä¸ºç©ºï¼Œç©ºæ—¶ä»…ä¿å­˜åˆ°å†…å­˜ï¼‰
    - title/result/type_tag: å±•ç¤ºç”¨çš„æ ‡é¢˜ä¸ HTML ç»“æœ
    - structured: å¯é€‰çš„ç»“æ„åŒ–ç»“æœï¼ˆPython åˆ—è¡¨/å­—å…¸ï¼‰ï¼Œä¼šåºåˆ—åŒ–åˆ° result_json

    æ³¨æ„ï¼šä¸è¦åœ¨æ­¤å‡½æ•°å†…éƒ¨è®¿é—® Flask sessionï¼Œ
    éœ€è¦åœ¨è°ƒç”¨æ–¹æŠŠ user_id æ˜¾å¼ä¼ å…¥ï¼Œä»¥ä¾¿åœ¨çº¿ç¨‹ä¸­å®‰å…¨è°ƒç”¨ã€‚
    è¿”å›ï¼šæ•°æ®åº“è®°å½• IDï¼ˆæˆåŠŸä¸”æœ‰ user_id ä¸”è¡¨ç»“æ„æ”¯æŒæ—¶ï¼‰ï¼Œå¦åˆ™ None
    """
    try:
        record_id = None

        if not user_id:
            logger.warning("âš ï¸ æœªæä¾› user_idï¼Œä»…ä¿å­˜å†…å­˜å†å²è®°å½•")
        else:
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼Œå¹¶è¿”å›è®°å½• ID
            try:
                record_id = db.execute_and_fetch_id("""
                    INSERT INTO analysis_results (user_id, title, result, type)
                    VALUES (%s, %s, %s, %s)
                    RETURNING id
                """, (user_id, title, result, type_tag))
                logger.info(f"ğŸ’¾ å·²ä¿å­˜å†å²è®°å½•åˆ°æ•°æ®åº“: {title} (id={record_id})")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜å†å²è®°å½•åˆ°æ•°æ®åº“å¤±è´¥: {e}")

            # è‹¥æä¾›äº†ç»“æ„åŒ–ç»“æœä¸”è¡¨ç»“æ„æ”¯æŒï¼Œå°è¯•å†™å…¥ result_json
            if record_id and structured is not None and ANALYSIS_RESULTS_HAS_JSON:
                try:
                    db.execute("""
                        UPDATE analysis_results
                        SET result_json = %s
                        WHERE id = %s
                    """, (json.dumps(structured, ensure_ascii=False), record_id))
                    logger.info(f"ğŸ’¾ å·²ä¸ºè®°å½• {record_id} å†™å…¥ç»“æ„åŒ–ç»“æœ result_json")
                except Exception as e:
                    logger.warning(f"âš ï¸ å†™å…¥ result_json å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨ HTML ç»“æœ: {e}")

        # åŒæ—¶ä¿å­˜åˆ°å†…å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
        record = {
            'id': len(HISTORY_DB) + 1,
            'title': f"{title} [{datetime.datetime.now().strftime('%H:%M')}]",
            'result': result,
            'type': type_tag
        }
        HISTORY_DB.append(record)

        return record_id

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è‡³å°‘ä¿å­˜åˆ°å†…å­˜
        record = {
            'id': len(HISTORY_DB) + 1,
            'title': f"{title} [{datetime.datetime.now().strftime('%H:%M')}]",
            'result': result,
            'type': type_tag
        }
        HISTORY_DB.append(record)
        return None


def call_veo_api(prompt):
    """è°ƒç”¨ Google Veo APIï¼ˆæ¨¡æ‹Ÿï¼‰"""
    logger.info(f"ğŸ¬ æ¨¡æ‹Ÿ Veo API è°ƒç”¨: {prompt[:50]}...")
    time.sleep(3)
    return "https://cdn.pixabay.com/video/2023/10/22/186115-877653483_large.mp4"


def log_usage(user_id, username, department, function_type, comments_count, ai_tokens):
    """è®°å½•ä½¿ç”¨æƒ…å†µå’Œæˆæœ¬"""
    try:
        # è®¡ç®—æˆæœ¬
        ai_cost = ai_tokens * 0.008 / 1000  # é€šä¹‰åƒé—®å®šä»·

        # æ ¹æ®åŠŸèƒ½ç±»å‹è®¡ç®— Apify æˆæœ¬
        if function_type == 'sentiment':
            # Facebook è¯„è®ºï¼š$2.50/1000æ¡
            apify_cost_usd = comments_count * 2.50 / 1000
        elif function_type == 'competitor':
            # TikTok æ•°æ®ï¼š$3.70/1000æ¡
            apify_cost_usd = comments_count * 3.70 / 1000
        else:
            apify_cost_usd = 0

        apify_cost = apify_cost_usd * USD_TO_CNY  # è½¬æ¢ä¸ºäººæ°‘å¸
        total_cost = ai_cost + apify_cost

        # ä¿å­˜åˆ°æ•°æ®åº“
        db.execute("""
            INSERT INTO usage_logs
            (user_id, username, department, function_type, comments_count,
             ai_tokens, ai_cost, apify_cost, total_cost)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (user_id, username, department, function_type, comments_count,
              ai_tokens, ai_cost, apify_cost, total_cost))

        logger.info(f"ğŸ’° æˆæœ¬è®°å½•: AI={ai_cost:.4f}å…ƒ + Apify={apify_cost:.4f}å…ƒ = æ€»è®¡{total_cost:.4f}å…ƒ")

        return total_cost

    except Exception as e:
        logger.error(f"âŒ è®°å½•ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
        return 0


def process_analysis_task(task_id, url, file_data, session_id, user_id, username, department, project='CFL'):
    """å¼‚æ­¥å¤„ç†åˆ†æä»»åŠ¡ã€‚project ä¸º CFL/PUBGM/HOKï¼Œç”¨äºé€‰æ‹©æç¤ºè¯ã€‚"""
    # ç”¨æˆ·ä¿¡æ¯å·²ä»ä¸»çº¿ç¨‹ä¼ å…¥ï¼Œä¸å†ä» session è·å–

    logger.info(f"ğŸ”„ åå°çº¿ç¨‹å·²å¯åŠ¨ï¼Œä»»åŠ¡ID: {task_id}ï¼Œé¡¹ç›®: {project}")
    logger.info(f"ğŸ‘¤ ç”¨æˆ·ä¿¡æ¯: user_id={user_id}, username={username}, department={department}")
    logger.info(f"ğŸ“‹ ä»»åŠ¡å‚æ•°: url={url}, has_file={file_data is not None}")

    # åœ¨çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„ Apify å®¢æˆ·ç«¯ï¼ˆé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰
    thread_apify_client = None
    if APIFY_TOKEN:
        try:
            logger.info("ğŸ”§ åœ¨åå°çº¿ç¨‹ä¸­åˆå§‹åŒ– Apify å®¢æˆ·ç«¯...")
            thread_apify_client = ApifyClient(
                token=APIFY_TOKEN,
                max_retries=3,
                min_delay_between_retries_millis=500
            )
            logger.info("âœ… çº¿ç¨‹ Apify å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ Apify å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            update_task(task_id, status='failed', error=f"Apify å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return

    # è¿½è¸ªæˆæœ¬æ•°æ®
    total_tokens = 0
    total_comments = 0

    try:
        logger.info(f"ğŸ“ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º processing...")
        update_task(task_id, status='processing', progress='æ­£åœ¨åˆå§‹åŒ–...')
        logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€æ›´æ–°æˆåŠŸ")

        content = ""
        img = None
        source_title = "æœªçŸ¥"

        # è·¯å¾„ A: æ–‡ä»¶ä¸Šä¼ åˆ†æ
        if file_data:
            update_task(task_id, progress='æ­£åœ¨å¤„ç†æ–‡ä»¶...')
            mode, res = process_uploaded_file(file_data)

            if mode == "ERROR":
                update_task(task_id, status='failed', error=res)
                return

            if mode == "IMAGE":
                img = res
                content = "åˆ†æå›¾ç‰‡ä¸­çš„åé¦ˆå†…å®¹"
            else:
                content = res

            source_title = f"æ–‡ä»¶: {file_data.filename[:15]}"

        # è·¯å¾„ B: ç¤¾äº¤åª’ä½“é“¾æ¥æŠ“å–åˆ†æ
        elif url:
            logger.info(f"ğŸŒ å¼€å§‹å¤„ç† URL: {url}")
            update_task(task_id, progress='æ­£åœ¨æŠ“å–ç¤¾åª’æ•°æ®...')

            if not thread_apify_client:
                logger.error("âŒ Apify å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                update_task(task_id, status='failed', error="APIFY_TOKEN æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥")
                return

            try:
                logger.info("ğŸ“‹ å‡†å¤‡ Apify çˆ¬è™«å‚æ•°...")
                run_input = {
                    "startUrls": [{"url": url}],
                    "resultsLimit": 1000,
                    "maxComments": 1000,
                    "maxPostCount": 1,
                    "maxCommentsPerPost": 1000,
                    "maxRepliesPerComment": 0,
                    "scrapeCommentReplies": False
                }

                logger.info("ğŸš€ å¯åŠ¨ Apify çˆ¬è™«...")
                logger.info("ğŸ“ æ­£åœ¨è°ƒç”¨ Apify REST API...")
                logger.info(f"   Actor: apify/facebook-comments-scraper")
                logger.info(f"   Input: {run_input}")

                try:
                    # ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨ Apify REST APIï¼ˆå¸¦è¶…æ—¶ï¼‰
                    start_time = time.time()
                    api_url = "https://api.apify.com/v2/acts/apify~facebook-comments-scraper/runs"
                    headers = {
                        "Authorization": f"Bearer {APIFY_TOKEN}",
                        "Content-Type": "application/json"
                    }

                    logger.info(f"   API URL: {api_url}")
                    logger.info(f"   ä½¿ç”¨ requests åº“ï¼Œè¶…æ—¶: 30ç§’")

                    response = requests.post(
                        api_url,
                        json=run_input,
                        headers=headers,
                        timeout=30  # 30 ç§’è¶…æ—¶
                    )

                    elapsed = time.time() - start_time
                    logger.info(f"âœ… HTTP è¯·æ±‚å®Œæˆï¼ˆè€—æ—¶: {elapsed:.2f}ç§’ï¼‰")
                    logger.info(f"   çŠ¶æ€ç : {response.status_code}")

                    if response.status_code != 201:
                        raise ValueError(f"Apify API è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")

                    run = response.json()['data']
                    logger.info(f"âœ… Apify API è¿”å›æˆåŠŸ")
                    logger.info(f"   è¿”å›ç±»å‹: {type(run)}")
                    logger.info(f"   Run ID: {run.get('id') if run else 'None'}")

                    if not run or 'id' not in run:
                        raise ValueError(f"Apify è¿”å›æ— æ•ˆ: {run}")

                    logger.info(f"âœ… çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼ŒRun ID: {run['id']}")

                except requests.Timeout:
                    error_msg = "Apify API è°ƒç”¨è¶…æ—¶ï¼ˆ30ç§’ï¼‰"
                    logger.error(f"âŒ {error_msg}")
                    update_task(task_id, status='failed', error=error_msg)
                    return
                except Exception as start_error:
                    error_msg = f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(start_error)}"
                    logger.error(f"âŒ {error_msg}")
                    logger.error(f"   é”™è¯¯ç±»å‹: {type(start_error).__name__}")
                    import traceback
                    logger.error(f"   å †æ ˆ:\n{traceback.format_exc()}")
                    update_task(task_id, status='failed', error=error_msg)
                    return

                logger.info("â³ ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆæœ€é•¿ 480 ç§’ï¼‰...")
                update_task(task_id, progress='ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆçº¦30-60ç§’ï¼‰...')

                try:
                    logger.info("ğŸ“¡ å¼€å§‹è½®è¯¢ Apify ä»»åŠ¡çŠ¶æ€...")
                    start_time = time.time()
                    max_wait_time = 480  # æœ€å¤šç­‰å¾… 480 ç§’
                    poll_interval = 5  # æ¯ 5 ç§’è½®è¯¢ä¸€æ¬¡

                    run_id = run['id']
                    api_url = f"https://api.apify.com/v2/actor-runs/{run_id}"
                    headers = {"Authorization": f"Bearer {APIFY_TOKEN}"}

                    while True:
                        elapsed = time.time() - start_time
                        if elapsed > max_wait_time:
                            raise TimeoutError(f"ç­‰å¾…çˆ¬è™«å®Œæˆè¶…æ—¶ï¼ˆ{max_wait_time}ç§’ï¼‰")

                        # è½®è¯¢ä»»åŠ¡çŠ¶æ€
                        logger.info(f"   è½®è¯¢çŠ¶æ€... (å·²ç­‰å¾… {elapsed:.0f}ç§’)")
                        response = requests.get(api_url, headers=headers, timeout=10)

                        if response.status_code != 200:
                            raise ValueError(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.status_code}")

                        run_data = response.json()['data']
                        status = run_data['status']

                        logger.info(f"   å½“å‰çŠ¶æ€: {status}")

                        if status in ['SUCCEEDED', 'FAILED', 'ABORTED', 'TIMED-OUT']:
                            # ä»»åŠ¡å®Œæˆ
                            run = run_data
                            break

                        # ç­‰å¾…åç»§ç»­è½®è¯¢
                        time.sleep(poll_interval)

                    elapsed = time.time() - start_time
                    logger.info(f"âœ… çˆ¬è™«å®Œæˆï¼ŒçŠ¶æ€: {run['status']}ï¼Œè€—æ—¶: {elapsed:.1f}ç§’")

                except requests.Timeout:
                    error_msg = "è½®è¯¢ä»»åŠ¡çŠ¶æ€è¶…æ—¶"
                    logger.error(f"âŒ {error_msg}")
                    update_task(task_id, status='failed', error=error_msg)
                    return
                except TimeoutError as timeout_error:
                    error_msg = str(timeout_error)
                    logger.error(f"âŒ {error_msg}")
                    update_task(task_id, status='failed', error=error_msg)
                    return
                except Exception as wait_error:
                    elapsed = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = f"ç­‰å¾…çˆ¬è™«å®Œæˆå¤±è´¥ï¼ˆè€—æ—¶ {elapsed:.1f}ç§’ï¼‰: {str(wait_error)}"
                    logger.error(f"âŒ {error_msg}")
                    update_task(task_id, status='failed', error=error_msg)
                    return

                if run['status'] != 'SUCCEEDED':
                    logger.error(f"âŒ çˆ¬è™«ä»»åŠ¡å¤±è´¥: {run['status']}")
                    update_task(task_id, status='failed', error=f"çˆ¬è™«ä»»åŠ¡å¤±è´¥: {run['status']}")
                    return

                # è·å–æ•°æ®
                logger.info("ğŸ“¦ å¼€å§‹è·å–çˆ¬è™«æ•°æ®...")
                dataset_id = run.get("defaultDatasetId")
                if not dataset_id:
                    error_msg = "æœªæ‰¾åˆ° dataset ID"
                    logger.error(f"âŒ {error_msg}")
                    update_task(task_id, status='failed', error=error_msg)
                    return

                # ä½¿ç”¨ REST API è·å–æ•°æ®
                dataset_url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
                try:
                    response = requests.get(dataset_url, headers=headers, timeout=30)
                    if response.status_code != 200:
                        raise ValueError(f"è·å–æ•°æ®å¤±è´¥: {response.status_code}")
                    items = response.json()
                    logger.info(f"âœ… æ€»å…±è·å–åˆ° {len(items)} æ¡æ•°æ®")
                except Exception as e:
                    error_msg = f"è·å–æ•°æ®å¤±è´¥: {str(e)}"
                    logger.error(f"âŒ {error_msg}")
                    update_task(task_id, status='failed', error=error_msg)
                    return
                total_comments = len(items)  # è®°å½•è¯„è®ºæ•°

                if not items:
                    update_task(task_id, status='failed', error="æœªå‘ç°å…¬å¼€è¯„è®º")
                    return

                # æŒ‰é¡¹ç›®å–æç¤ºè¯æ¨¡æ¿ï¼›ç©ºåˆ™ç›´æ¥å¤±è´¥
                sentiment_template = get_prompt('sentiment', project)
                if not sentiment_template:
                    update_task(task_id, status='failed', error='è¯¥é¡¹ç›®æç¤ºè¯å°šæœªé…ç½®')
                    return

                # åˆ†æ‰¹å¤„ç†è¯„è®º
                batch_size = 50
                all_results = []
                total_batches = (len(items) + batch_size - 1) // batch_size

                for i in range(0, len(items), batch_size):
                    batch = items[i:i+batch_size]
                    batch_num = i // batch_size + 1

                    update_task(task_id, progress=f'AI åˆ†æä¸­ï¼šç¬¬ {batch_num}/{total_batches} æ‰¹...')
                    logger.info(f"ğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ï¼ˆ{len(batch)} æ¡è¯„è®ºï¼‰...")

                    batch_content = "\n".join([f"ç”¨æˆ·{j}: {it.get('text', '')}" for j, it in enumerate(batch)])
                    batch_prompt = sentiment_template.format(batch_content=batch_content)

                    result, tokens = call_gemini(batch_prompt, timeout=60)
                    total_tokens += tokens

                    # è§£æ JSON ç»“æœ
                    try:
                        import json
                        import re
                        clean_result = re.sub(r'```json\\s*|\\s*```', '', result).strip()
                        batch_data = json.loads(clean_result)
                        all_results.extend(batch_data)
                        logger.info(f"âœ… ç¬¬ {batch_num} æ‰¹å®Œæˆï¼Œè·å¾— {len(batch_data)} æ¡æœ‰æ•ˆç»“æœ")
                    except Exception as e:
                        logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹è§£æå¤±è´¥: {e}")
                        continue

                # ç”Ÿæˆ HTML è¡¨æ ¼
                update_task(task_id, progress='ç”ŸæˆæŠ¥å‘Š...')
                logger.info(f"ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œå…± {len(all_results)} æ¡æœ‰æ•ˆè¯„è®º...")

                # æŒ‰åˆ†ç±»æ’åº
                category_order = ["å¤–æŒ‚ä½œå¼Š", "æ¸¸æˆä¼˜åŒ–", "æ¸¸æˆBug", "å……å€¼é€€æ¬¾", "æ–°æ¨¡å¼/åœ°å›¾/å¹³è¡¡æ€§å»ºè®®", "å…¶ä»–"]
                all_results.sort(key=lambda x: category_order.index(x.get('category', 'å…¶ä»–')) if x.get('category') in category_order else len(category_order))

                # ç”Ÿæˆ HTML
                html_rows = []
                for idx, item in enumerate(all_results, 1):
                    html_rows.append(f"""
                    <tr>
                        <td>{idx}</td>
                        <td style="white-space: pre-wrap; word-break: break-word;">{item.get('text', '')}</td>
                        <td><strong>{item.get('category', '')}</strong></td>
                        <td>{item.get('sentiment', '')}</td>
                        <td>{item.get('language', '')}</td>
                        <td style="white-space: pre-wrap; word-break: break-word;">{item.get('analysis', '')}</td>
                    </tr>
                    """)

                result = f"""
                <table class="table table-hover">
                    <thead>
                        <tr>
                            <th style="width:40px;">#</th>
                            <th style="width:25%;">åŸå§‹è¯„è®º</th>
                            <th style="width:100px;">å½’ç±»</th>
                            <th style="width:70px;">æƒ…æ„Ÿ</th>
                            <th style="width:60px;">è¯­è¨€</th>
                            <th>ç®€è¦åˆ†æ</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join(html_rows)}
                    </tbody>
                </table>
                """

                # ä¿å­˜ç»“æœç”¨äºå¯¼å‡ºï¼ˆåç»­å°†æ”¹ä¸ºä»æ•°æ®åº“è¯»å–ï¼‰
                LATEST_ANALYSIS_RESULTS[session_id] = all_results
                source_title = f"FB: {url[:15]}..."

            except Exception as e:
                error_msg = f"çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
                import traceback
                logger.error(f"âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")

                update_task(task_id, status='failed', error=error_msg)
                return

        else:
            update_task(task_id, status='failed', error="è¯·æä¾›é“¾æ¥æˆ–æ–‡ä»¶")
            return

        # ä¿å­˜å†å²è®°å½•ï¼ˆåŒæ—¶å†™å…¥ç»“æ„åŒ–ç»“æœï¼Œä¾¿äºåç»­å¯¼å‡ºä»»æ„å†å²è®°å½•ï¼‰
        save_history(user_id, source_title, result, 'sentiment', structured=all_results)

        # è®°å½•ä½¿ç”¨æˆæœ¬
        if user_id:
            log_usage(user_id, username, department, 'sentiment', total_comments, total_tokens)

        # ä»»åŠ¡å®Œæˆ
        update_task(task_id, status='completed', result=result, progress='åˆ†æå®Œæˆï¼')
        logger.info(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ")

    except Exception as e:
        error_msg = f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
        logger.error(f"âŒ ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
        logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")

        update_task(task_id, status='failed', error=error_msg, progress='ä»»åŠ¡å¤±è´¥')

# ============================================
# åŸºç¡€è·¯ç”±
# ============================================

@app.route('/login', methods=['GET', 'POST'])
def login():
    """ç™»å½•é¡µé¢"""
    if request.method == 'POST':
        username = request.form.get('username')
        password = request.form.get('password')

        try:
            # ä»æ•°æ®åº“æŸ¥è¯¢ç”¨æˆ·
            user = db.query_one(
                "SELECT * FROM users WHERE username = %s",
                (username,)
            )

            if user and bcrypt.check_password_hash(user['password_hash'], password):
                session['logged_in'] = True
                session['user_id'] = user['id']
                session['username'] = user['username']
                session['real_name'] = user['real_name']
                session['department'] = user['department']
                session['role'] = user['role']
                session['session_id'] = f"{username}_{int(time.time())}"
                logger.info(f"âœ… ç”¨æˆ·ç™»å½•æˆåŠŸ: {username} ({user['real_name']})")
                return redirect(url_for('home'))
            else:
                logger.info(f"âŒ ç™»å½•å¤±è´¥: {username}")
                return render_template('login.html', error='ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯')

        except Exception as e:
            logger.error(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return render_template('login.html', error='ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•')

    return render_template('login.html')


@app.route('/logout')
def logout():
    """ç™»å‡º"""
    username = session.get('username', 'æœªçŸ¥ç”¨æˆ·')
    session.clear()
    logger.info(f"ğŸ‘‹ ç”¨æˆ·å·²ç™»å‡º: {username}")
    return redirect(url_for('login'))


@app.route('/')
@login_required
def home():
    """é¦–é¡µ"""
    return render_template('index.html', user=session)


@app.route('/dashboard_stats')
@login_required
def dashboard_stats():
    """é¦–é¡µæ•°æ®çœ‹æ¿ API"""
    try:
        current_month = datetime.datetime.now().strftime('%Y-%m')

        # æœ¬æœˆæ•°æ®
        current_data = db.query_one("""
            SELECT
                COALESCE(SUM(comments_count), 0) as total_comments,
                COUNT(*) as total_analyses,
                COALESCE(SUM(ai_tokens), 0) as total_tokens
            FROM usage_logs
            WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
        """, (current_month,))

        # ä¸Šæœˆæ•°æ®ï¼ˆç”¨äºè®¡ç®—å¢é•¿ç‡ï¼‰
        last_month = (datetime.datetime.now().replace(day=1) - datetime.timedelta(days=1)).strftime('%Y-%m')
        last_data = db.query_one("""
            SELECT COALESCE(SUM(comments_count), 0) as total_comments
            FROM usage_logs
            WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
        """, (last_month,))

        # è®¡ç®—å¢é•¿ç‡
        growth = 0
        if last_data and last_data['total_comments'] > 0:
            growth = ((current_data['total_comments'] - last_data['total_comments']) / last_data['total_comments']) * 100

        return jsonify({
            'comments': int(current_data['total_comments']),
            'analyses': int(current_data['total_analyses']),
            'tokens': int(current_data['total_tokens']),
            'growth': round(growth, 1)
        })

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®çœ‹æ¿å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤å€¼
        return jsonify({
            'comments': 0,
            'analyses': 0,
            'tokens': 0,
            'growth': 0
        })


@app.route('/debug')
@login_required
def debug_page():
    """è°ƒè¯•é¡µé¢"""
    debug_info = {
        "status": "Online",
        "qwen_key": bool(DASHSCOPE_API_KEY),
        "apify_key": bool(APIFY_TOKEN),
        "port": PORT,
        "history_count": len(HISTORY_DB)
    }
    logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯: {debug_info}")
    return jsonify(debug_info)


@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ç”¨äº Render ç›‘æ§"""
    return jsonify({"status": "ok", "service": "Sailson AI"}), 200


@app.route('/submit_feedback', methods=['POST'])
def submit_feedback():
    """æ¥æ”¶ç”¨æˆ·åé¦ˆå¹¶å‘é€é‚®ä»¶"""
    try:
        data = request.json
        project_name = data.get('project_name')
        feedback = data.get('feedback')

        if not project_name or not feedback:
            return jsonify({'error': 'è¯·å¡«å†™å®Œæ•´ä¿¡æ¯'}), 400

        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"ğŸ“§ æ”¶åˆ°ç”¨æˆ·åé¦ˆ")
        logger.info(f"   é¡¹ç›®åç§°: {project_name}")
        logger.info(f"   åé¦ˆå†…å®¹: {feedback}")

        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
        try:
            db.execute("""
                INSERT INTO feedback (user_email, content, created_at)
                VALUES (%s, %s, NOW())
            """, (project_name, feedback))
        except Exception as db_error:
            # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåªè®°å½•æ—¥å¿—
            logger.warning(f"âš ï¸ ä¿å­˜åé¦ˆåˆ°æ•°æ®åº“å¤±è´¥ï¼ˆè¡¨å¯èƒ½ä¸å­˜åœ¨ï¼‰: {db_error}")

        # å‘é€é‚®ä»¶é€šçŸ¥ç®¡ç†å‘˜ï¼ˆå¦‚æœé…ç½®äº† SMTPï¼‰
        email_sent = send_feedback_email(project_name, feedback)

        msg = 'æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼'
        if not email_sent:
            # ä¸æ‰“æ‰°ç”¨æˆ·ï¼Œåªåœ¨æ—¥å¿—ä¸­è®°å½•é‚®ä»¶å¤±è´¥
            logger.warning("âš ï¸ åé¦ˆå·²ä¿å­˜ï¼Œä½†é‚®ä»¶é€šçŸ¥æœªæˆåŠŸå‘é€")

        return jsonify({'success': True, 'message': msg})

    except Exception as e:
        logger.error(f"âŒ å¤„ç†åé¦ˆå¤±è´¥: {e}")
        return jsonify({'error': 'ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•'}), 500


# ============================================
# åŠŸèƒ½ 1: èˆ†æƒ…åˆ†æ
# ============================================

@app.route('/sentiment-tool')
@login_required
def sentiment_tool():
    """èˆ†æƒ…åˆ†æå·¥å…·é¡µé¢"""
    user_id = session.get('user_id')

    has_used_sentiment = False
    if user_id:
        try:
            row = db.query_one(
                """
                SELECT 1 FROM analysis_results
                WHERE user_id = %s AND type = %s
                LIMIT 1
                """,
                (user_id, 'sentiment')
            )
            has_used_sentiment = bool(row)
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥èˆ†æƒ…å†å²è®°å½•å¤±è´¥: {e}")

    return render_template('analysis.html', has_used_sentiment=has_used_sentiment)


@app.route('/analyze', methods=['POST'])
def analyze():
    """èˆ†æƒ…åˆ†æ API - å¼‚æ­¥ç‰ˆæœ¬"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ”¶åˆ°èˆ†æƒ…åˆ†æè¯·æ±‚")
    logger.info(f"ğŸ”‘ DASHSCOPE_API_KEY: {'âœ…' if DASHSCOPE_API_KEY else 'âŒ'}")
    logger.info(f"ğŸ”‘ APIFY_TOKEN: {'âœ…' if APIFY_TOKEN else 'âŒ'}")

    url = request.form.get('url')
    file = request.files.get('file')
    project = (request.form.get('project') or 'CFL').strip().upper()
    if project not in VALID_PROJECTS:
        project = 'CFL'

    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = str(uuid.uuid4())
    session_id = session.get('session_id', 'default')

    # åœ¨ä¸»çº¿ç¨‹ä¸­æå–ç”¨æˆ·ä¿¡æ¯ï¼ˆé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰
    user_id = session.get('user_id')
    username = session.get('username', 'unknown')
    department = session.get('department', 'æœªçŸ¥')

    # åœ¨ä¸»çº¿ç¨‹ä¸­è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆé¿å…è·¨çº¿ç¨‹è®¿é—® Flask FileStorage å¯¹è±¡ï¼‰
    file_data = None
    if file:
        try:
            file_data = {
                'filename': file.filename,
                'content': file.read(),  # è¯»å–æ–‡ä»¶å†…å®¹åˆ°å†…å­˜
                'content_type': file.content_type
            }
            logger.info(f"ğŸ“ å·²è¯»å–æ–‡ä»¶: {file.filename}, å¤§å°: {len(file_data['content'])} å­—èŠ‚")
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return jsonify({'error': f'è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'}), 400

    # åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“ï¼ˆæ ‡è®°ç±»å‹ä¸º sentimentï¼‰
    create_task(task_id, user_id, session_id, function_type='sentiment')

    # ç›®å‰ä»ç”± Web è¿›ç¨‹å†…çº¿ç¨‹æ‰§è¡Œé•¿ä»»åŠ¡ï¼Œåç»­å¯é€šè¿‡ USE_DB_WORKER åˆ‡æ¢åˆ°ç‹¬ç«‹ worker
    if not USE_DB_WORKER:
        thread = threading.Thread(
            target=process_analysis_task,
            args=(task_id, url, file_data, session_id, user_id, username, department, project)
        )
        # ä¸è®¾ç½® daemon=Trueï¼Œè®©çº¿ç¨‹è‡ªç„¶å®Œæˆï¼Œé¿å…è¢« Flask è¯·æ±‚ç»“æŸæ—¶æ€æ­»
        thread.start()
        logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²åˆ›å»ºå¹¶åœ¨æœ¬è¿›ç¨‹ä¸­å¯åŠ¨")
    else:
        logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²åˆ›å»ºï¼Œç­‰å¾…å¤–éƒ¨ worker å¤„ç†")

    # ç«‹å³è¿”å›ä»»åŠ¡ ID
    return jsonify({
        'task_id': task_id,
        'status': 'pending',
        'message': 'ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°å¤„ç†...'
    })


@app.route('/task_status/<task_id>')
def task_status(task_id):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    task = get_task(task_id)

    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

    return jsonify({
        'status': task['status'],
        'progress': task['progress'],
        'result': task['result'],
        'error': task['error']
    })

# ============================================
# åŠŸèƒ½ 2: ç«å“ç›‘æ§
# ============================================

@app.route('/competitor-tool')
@login_required
def competitor_tool():
    """ç«å“ç›‘æ§å·¥å…·é¡µé¢"""
    user_id = session.get('user_id')

    has_used_competitor = False
    if user_id:
        try:
            row = db.query_one(
                """
                SELECT 1 FROM analysis_results
                WHERE user_id = %s AND type = %s
                LIMIT 1
                """,
                (user_id, 'competitor')
            )
            has_used_competitor = bool(row)
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç«å“å†å²è®°å½•å¤±è´¥: {e}")

    return render_template('competitor.html', has_used_competitor=has_used_competitor)


@app.route('/monitor_competitors', methods=['POST'])
def monitor_competitors():
    """ç«å“ç›‘æ§ API - å¼‚æ­¥ç‰ˆæœ¬"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ”¶åˆ°ç«å“ç›‘æ§è¯·æ±‚")

    try:
        data = request.json
        target_url = data.get('competitor_name')
        start_dt_str = data.get('startDate')
        end_dt_str = data.get('endDate')
        project = (data.get('project') or 'CFL').strip().upper()
        if project not in VALID_PROJECTS:
            project = 'CFL'

        logger.info(f"ğŸ¯ ç›®æ ‡ URL: {target_url}")
        logger.info(f"ğŸ“… æ—¶é—´æ®µ: {start_dt_str} ~ {end_dt_str}ï¼Œé¡¹ç›®: {project}")

        if not APIFY_TOKEN:
            error_msg = "âŒ é”™è¯¯ï¼šAPIFY_TOKEN æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨çˆ¬è™«åŠŸèƒ½"
            logger.error(error_msg)
            return jsonify({'error': error_msg}), 400

        # è·å–ç”¨æˆ·ä¿¡æ¯
        user_id = session.get('user_id')
        username = session.get('username', 'unknown')
        department = session.get('department', 'æœªçŸ¥')
        session_id = session.get('session_id', str(uuid.uuid4()))

        # åˆ›å»ºä»»åŠ¡ ID
        task_id = str(uuid.uuid4())

        # åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“ï¼ˆæ ‡è®°ç±»å‹ä¸º competitorï¼‰
        create_task(task_id, user_id, session_id, function_type='competitor')

        # ç›®å‰ä»ç”± Web è¿›ç¨‹å†…çº¿ç¨‹æ‰§è¡Œé•¿ä»»åŠ¡ï¼Œåç»­å¯é€šè¿‡ USE_DB_WORKER åˆ‡æ¢åˆ°ç‹¬ç«‹ worker
        if not USE_DB_WORKER:
            thread = threading.Thread(
                target=process_competitor_task,
                args=(task_id, target_url, start_dt_str, end_dt_str, user_id, username, department, session_id, project)
            )
            # ä¸è®¾ç½® daemon=Trueï¼Œè®©çº¿ç¨‹è‡ªç„¶å®Œæˆ
            thread.start()
            logger.info(f"âœ… ç«å“ç›‘æ§ä»»åŠ¡ {task_id} å·²åˆ›å»ºå¹¶åœ¨æœ¬è¿›ç¨‹ä¸­å¯åŠ¨")
        else:
            logger.info(f"âœ… ç«å“ç›‘æ§ä»»åŠ¡ {task_id} å·²åˆ›å»ºï¼Œç­‰å¾…å¤–éƒ¨ worker å¤„ç†")

        # ç«‹å³è¿”å›ä»»åŠ¡ ID
        return jsonify({
            'task_id': task_id,
            'status': 'pending',
            'message': 'ç«å“ç›‘æ§ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·ç¨åæŸ¥çœ‹ç»“æœ'
        })

    except Exception as e:
        error_msg = f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        import traceback
        traceback.print_exc()
        return jsonify({'error': error_msg}), 500


def process_competitor_task(task_id, target_url, start_dt_str, end_dt_str, user_id, username, department, session_id, project='CFL'):
    """åå°å¤„ç†ç«å“ç›‘æ§ä»»åŠ¡ã€‚project ä¸º CFL/PUBGM/HOKï¼Œç”¨äºé€‰æ‹©æç¤ºè¯ã€‚"""
    try:
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†ç«å“ç›‘æ§ä»»åŠ¡ {task_id}")
        update_task(task_id, status='processing', progress='æ­£åœ¨åˆå§‹åŒ–...')

        # 1. æ—¥æœŸè½¬æ¢
        target_start = datetime.datetime.strptime(start_dt_str, '%Y-%m-%d').date()
        target_end = datetime.datetime.strptime(end_dt_str, '%Y-%m-%d').date()
        logger.info(f"ğŸ“† è§£ææ—¥æœŸ: {target_start} ~ {target_end}")

        # 2. äº‘ç«¯æŠ“å–
        logger.info("ğŸ•µï¸ å¯åŠ¨ TikTok çˆ¬è™«...")
        update_task(task_id, progress='æ­£åœ¨å¯åŠ¨ TikTok çˆ¬è™«...')

        run_input = {
            "profiles": [target_url],
            "resultsPerPage": 35,
            "oldestPostDate": start_dt_str,
            "shouldDownloadVideos": False
        }

        try:
            # ä½¿ç”¨ REST API å¯åŠ¨çˆ¬è™«
            logger.info("ğŸ“ æ­£åœ¨è°ƒç”¨ Apify REST API...")
            logger.info(f"   Actor: clockworks/tiktok-scraper")
            logger.info(f"   Input: {run_input}")

            api_url = "https://api.apify.com/v2/acts/clockworks~tiktok-scraper/runs"
            headers = {
                "Authorization": f"Bearer {APIFY_TOKEN}",
                "Content-Type": "application/json"
            }

            response = requests.post(
                api_url,
                json=run_input,
                headers=headers,
                timeout=30
            )

            if response.status_code != 201:
                raise ValueError(f"Apify API è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")

            run = response.json()['data']
            logger.info(f"âœ… çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼ŒRun ID: {run['id']}")
        except requests.Timeout:
            error_msg = "Apify API è°ƒç”¨è¶…æ—¶ï¼ˆ30ç§’ï¼‰"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return
        except Exception as start_error:
            error_msg = f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(start_error)}"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return

        # ç­‰å¾…çˆ¬è™«å®Œæˆ
        logger.info("â³ ç­‰å¾…çˆ¬è™«å®Œæˆ...")
        update_task(task_id, progress='ç­‰å¾…çˆ¬è™«å®Œæˆï¼ˆçº¦30-60ç§’ï¼‰...')

        try:
            logger.info("ğŸ“¡ å¼€å§‹è½®è¯¢ TikTok çˆ¬è™«çŠ¶æ€...")
            start_time = time.time()
            max_wait_time = 480  # æœ€å¤šç­‰å¾… 480 ç§’
            poll_interval = 5  # æ¯ 5 ç§’è½®è¯¢ä¸€æ¬¡

            run_id = run['id']
            api_url = f"https://api.apify.com/v2/actor-runs/{run_id}"
            headers = {"Authorization": f"Bearer {APIFY_TOKEN}"}

            while True:
                elapsed = time.time() - start_time
                if elapsed > max_wait_time:
                    raise TimeoutError(f"ç­‰å¾…çˆ¬è™«å®Œæˆè¶…æ—¶ï¼ˆ{max_wait_time}ç§’ï¼‰")

                # è½®è¯¢ä»»åŠ¡çŠ¶æ€
                logger.info(f"   è½®è¯¢çŠ¶æ€... (å·²ç­‰å¾… {elapsed:.0f}ç§’)")
                response = requests.get(api_url, headers=headers, timeout=10)

                if response.status_code != 200:
                    raise ValueError(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.status_code}")

                run_data = response.json()['data']
                status = run_data['status']

                logger.info(f"   å½“å‰çŠ¶æ€: {status}")

                if status in ['SUCCEEDED', 'FAILED', 'ABORTED', 'TIMED-OUT']:
                    # ä»»åŠ¡å®Œæˆ
                    run = run_data
                    break

                # ç­‰å¾…åç»§ç»­è½®è¯¢
                time.sleep(poll_interval)

            elapsed = time.time() - start_time
            logger.info(f"âœ… çˆ¬è™«ä»»åŠ¡å®Œæˆï¼ŒçŠ¶æ€: {run['status']}ï¼Œè€—æ—¶: {elapsed:.1f}ç§’")

        except requests.Timeout:
            error_msg = "è½®è¯¢ä»»åŠ¡çŠ¶æ€è¶…æ—¶"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return
        except TimeoutError as timeout_error:
            error_msg = str(timeout_error)
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return
        except Exception as wait_error:
            error_msg = f"ç­‰å¾…çˆ¬è™«å®Œæˆå¤±è´¥: {str(wait_error)}"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return

        if run['status'] != 'SUCCEEDED':
            error_msg = f"çˆ¬è™«ä»»åŠ¡å¤±è´¥ï¼ŒçŠ¶æ€: {run['status']}"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return

        # è·å–æ•°æ®
        dataset_id = run.get("defaultDatasetId")
        if not dataset_id:
            error_msg = "æœªæ‰¾åˆ° dataset ID"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return

        # ä½¿ç”¨ REST API è·å–æ•°æ®
        dataset_url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
        try:
            response = requests.get(dataset_url, headers=headers, timeout=30)
            if response.status_code != 200:
                raise ValueError(f"è·å–æ•°æ®å¤±è´¥: {response.status_code}")
            items = response.json()
            logger.info(f"ğŸ“¦ è·å–åˆ° {len(items)} æ¡åŸå§‹æ•°æ®")
            update_task(task_id, progress=f'å·²è·å– {len(items)} æ¡æ•°æ®ï¼Œæ­£åœ¨è¿‡æ»¤...')
        except Exception as e:
            error_msg = f"è·å–æ•°æ®å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            update_task(task_id, status='failed', error=error_msg)
            return

        # 3. æœ¬åœ°æ—¶é—´è¿‡æ»¤
        cleaned = []
        for it in items:
            raw_date = it.get("createTimeISO")
            if not raw_date:
                continue

            post_dt = datetime.datetime.fromisoformat(raw_date.replace('Z', '+00:00')).date()

            if target_start <= post_dt <= target_end:
                cleaned.append({
                    "desc": it.get("text") or it.get("desc") or "æ— æè¿°",
                    "likes": it.get("diggCount", 0),
                    "views": it.get("playCount", 0),
                    "comments": it.get("commentCount", 0),
                    "shares": it.get("shareCount", 0),
                    "collects": it.get("collectCount", 0),
                    "url": it.get("webVideoUrl"),
                    "date": str(post_dt)
                })

        logger.info(f"âœ… æ—¶é—´è¿‡æ»¤åå‰©ä½™ {len(cleaned)} æ¡æ•°æ®")

        if not cleaned:
            warning_msg = f"åœ¨æ­¤æœŸé—´ ({start_dt_str} ~ {end_dt_str}) æœªå‘ç°è§†é¢‘ã€‚"
            logger.info("âš ï¸ æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘")
            update_task(task_id, status='completed', result=f"<div class='alert alert-warning'>{warning_msg}</div>")
            return

        # 4. é€šä¹‰åƒé—®ç”ŸæˆæŠ¥å‘Šï¼ˆæŒ‰é¡¹ç›®å–æç¤ºè¯ï¼‰
        competitor_template = get_prompt('competitor', project)
        if not competitor_template:
            update_task(task_id, status='failed', error='è¯¥é¡¹ç›®æç¤ºè¯å°šæœªé…ç½®')
            return

        update_task(task_id, progress='æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...')
        cleaned_str = json.dumps(cleaned, ensure_ascii=False)
        prompt = competitor_template.format(cleaned=cleaned_str, start_dt_str=start_dt_str, end_dt_str=end_dt_str)

        logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨é€šä¹‰åƒé—® API ç”ŸæˆæŠ¥å‘Š...")
        result, tokens = call_gemini(prompt)

        # æ¸…ç† Markdown ä»£ç å—æ ‡è®°
        result = result.replace('```html', '').replace('```', '').strip()

        # åœ¨æ€»è§ˆä¸‹æ–¹è¿½åŠ æŒ‰è§†é¢‘ç»´åº¦çš„æ˜ç»†è¡¨ï¼ˆæ ·å¼ä¸æ€»æ•°æ®è¡¨æ ¼ç»Ÿä¸€ï¼‰
        try:
            rows_html = []
            total_count = len(cleaned)
            # æ€»è®¡è¡Œï¼šæ—¥æœŸåˆ—æ˜¾ç¤ºæ€»æ¡æ•°ï¼Œæ ·å¼ä¸è¡¨å¤´ç»Ÿä¸€
            rows_html.append(f"""
            <tr style="background:#F8F9FA;">
                <td style="padding:15px 10px; border-bottom:2px solid #EEE; font-weight:600; color:#333;">å…± {total_count} æ¡</td>
                <td colspan="7" style="padding:15px 10px; border-bottom:2px solid #EEE;"></td>
            </tr>
            """)

            for item in cleaned:
                date_str = html.escape(item.get("date", "") or "")
                desc = html.escape(item.get("desc", "") or "æ— æè¿°")
                views = item.get("views", 0)
                likes = item.get("likes", 0)
                comments_cnt = item.get("comments", 0)
                collects = item.get("collects", 0)
                shares = item.get("shares", 0)
                url = item.get("url") or ""
                if url:
                    url_html = f'<a href="{html.escape(url)}" target="_blank" style="color:#D32F2F; font-weight:600;">æŸ¥çœ‹</a>'
                else:
                    url_html = "â€”"

                rows_html.append(f"""
                <tr>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{date_str}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:left; font-size:0.9rem; word-wrap:break-word; white-space:pre-wrap;">{desc}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{views}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{likes}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{comments_cnt}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{collects}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{shares}</td>
                    <td style="padding:15px 10px; vertical-align:middle; border-bottom:1px solid #F1F3F5; text-align:center; font-size:0.9rem;">{url_html}</td>
                </tr>
                """)

            per_video_table = f"""
<div style="margin-top:30px;">
<h3 style="color:#D32F2F; border-bottom:2px solid #eee; padding-bottom:10px; margin-bottom:15px;">
    ğŸ“º æ˜ç»†åˆ—è¡¨ï¼ˆæŒ‰è§†é¢‘ï¼‰
</h3>
<table style="width:100%; table-layout:fixed; border-collapse:collapse; margin:20px 0; border:1px solid #eee; border-radius:10px; overflow:hidden; font-size:0.9rem;">
    <thead>
        <tr>
            <th style="width:95px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">æ—¥æœŸ</th>
            <th style="background:#F8F9FA; padding:15px 10px; text-align:left; color:#666; font-weight:600; border-bottom:2px solid #EEE;">è§†é¢‘æè¿°</th>
            <th style="width:85px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">æ’­æ”¾</th>
            <th style="width:75px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">ç‚¹èµ</th>
            <th style="width:70px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">è¯„è®º</th>
            <th style="width:70px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">æ”¶è—</th>
            <th style="width:70px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">è½¬å‘</th>
            <th style="width:72px; background:#F8F9FA; padding:15px 10px; text-align:center; color:#666; font-weight:600; border-bottom:2px solid #EEE;">é“¾æ¥</th>
        </tr>
    </thead>
    <tbody>
        {''.join(rows_html)}
    </tbody>
</table>
</div>
"""
            result = f"{result}\n{per_video_table}"
        except Exception as build_table_error:
            logger.error(f"âš ï¸ æ„å»ºç«å“æ˜ç»†è¡¨å¤±è´¥: {build_table_error}")

        # ä¿å­˜å†å²è®°å½•
        save_history(user_id, f"ç«å“æ•°æ®:{target_url[20:30]}", result, 'competitor')

        # è®°å½•ä½¿ç”¨æˆæœ¬
        if user_id:
            log_usage(
                user_id,
                username,
                department,
                'competitor',
                len(cleaned),  # TikTok è§†é¢‘æ•°é‡
                tokens
            )

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        update_task(task_id, status='completed', result=result, progress='åˆ†æå®Œæˆ')

        logger.info("âœ… ç«å“ç›‘æ§å®Œæˆ")
        logger.info("=" * 60 + "\n")

    except Exception as e:
        error_msg = f"ç«å“ç›‘æ§å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        import traceback
        logger.error(traceback.format_exc())
        update_task(task_id, status='failed', error=error_msg)
        return jsonify({'result': f"<div class='alert alert-danger'>{error_msg}</div>"})

# ============================================
# åŠŸèƒ½ 3: è§†é¢‘ç”Ÿæˆ
# ============================================

@app.route('/video-tool')
@login_required
def video_tool():
    """è§†é¢‘ç”Ÿæˆå·¥å…·é¡µé¢"""
    return render_template('video.html')


@app.route('/generate_video', methods=['POST'])
def generate_video():
    """è§†é¢‘ç”Ÿæˆ API"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“¥ æ”¶åˆ°è§†é¢‘ç”Ÿæˆè¯·æ±‚")

    try:
        prompt = request.json.get('prompt')
        logger.info(f"ğŸ¬ Prompt: {prompt[:50]}...")

        video_url = call_veo_api(prompt)
        save_history(session.get('user_id'), f"è§†é¢‘: {prompt[:10]}", video_url, 'video')

        logger.info("âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ")
        logger.info("=" * 60 + "\n")

        return jsonify({'video_url': video_url})

    except Exception as e:
        error_msg = f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return jsonify({'error': error_msg})

# ============================================
# å†å²è®°å½•ç®¡ç†
# ============================================

@app.route('/get_history')
@login_required
def get_history():
    """è·å–å†å²è®°å½•ï¼ˆä»æ•°æ®åº“ï¼‰"""
    try:
        user_id = session.get('user_id')

        # ä»æ•°æ®åº“è¯»å–ç”¨æˆ·çš„å†å²è®°å½•
        records = db.query_all("""
            SELECT id, title, result, type, created_at
            FROM analysis_results
            WHERE user_id = %s
            ORDER BY created_at DESC
            LIMIT 50
        """, (user_id,))

        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        result = []
        for record in records:
            result.append({
                'id': record['id'],
                'title': f"{record['title']} [{record['created_at'].strftime('%H:%M')}]",
                'result': record['result'],
                'type': record['type']
            })

        return jsonify(result)

    except Exception as e:
        logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è¿”å›å†…å­˜ä¸­çš„è®°å½•
        return jsonify(HISTORY_DB[::-1])


@app.route('/get_record/<int:id>')
@login_required
def get_record(id):
    """è·å–å•æ¡è®°å½•ï¼ˆä»æ•°æ®åº“ï¼‰"""
    try:
        user_id = session.get('user_id')

        # ä»æ•°æ®åº“è¯»å–ï¼ˆç¡®ä¿åªèƒ½è¯»å–è‡ªå·±çš„è®°å½•ï¼‰
        record = db.query_one("""
            SELECT id, title, result, type, created_at
            FROM analysis_results
            WHERE id = %s AND user_id = %s
        """, (id, user_id))

        if record:
            return jsonify({
                'id': record['id'],
                'title': record['title'],
                'result': record['result'],
                'type': record['type']
            })
        else:
            return jsonify({'error': 'è®°å½•ä¸å­˜åœ¨'}), 404

    except Exception as e:
        logger.error(f"âŒ è·å–è®°å½•å¤±è´¥: {e}")
        # å¤±è´¥æ—¶ä»å†…å­˜æŸ¥æ‰¾
        record = next((x for x in HISTORY_DB if x['id'] == id), None)
        return jsonify(record) if record else jsonify({'error': 'è®°å½•ä¸å­˜åœ¨'}), 404

# ============================================
# Excel å¯¼å‡ºåŠŸèƒ½
# ============================================

def create_excel_by_language(results):
    """æŒ‰è¯­è¨€åˆ†ç±»ç”Ÿæˆ Excel"""
    wb = Workbook()
    wb.remove(wb.active)  # åˆ é™¤é»˜è®¤ sheet

    # æŒ‰è¯­è¨€åˆ†ç»„
    language_groups = {}
    for item in results:
        lang = item.get('language', 'å…¶ä»–')
        if lang not in language_groups:
            language_groups[lang] = []
        language_groups[lang].append(item)

    # ä¸ºæ¯ä¸ªè¯­è¨€åˆ›å»º Sheet
    for lang, items in sorted(language_groups.items()):
        ws = wb.create_sheet(title=lang)

        # è¡¨å¤´
        headers = ['åºå·', 'åŸå§‹è¯„è®º', 'å½’ç±»', 'æƒ…æ„Ÿå€¾å‘', 'è¯­è¨€', 'ç®€è¦åˆ†æ']
        ws.append(headers)

        # è®¾ç½®è¡¨å¤´æ ·å¼
        header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
        header_font = Font(bold=True, color='FFFFFF')
        for cell in ws[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center')

        # æ·»åŠ æ•°æ®
        for idx, item in enumerate(items, 1):
            ws.append([
                idx,
                item.get('text', ''),
                item.get('category', ''),
                item.get('sentiment', ''),
                item.get('language', ''),
                item.get('analysis', '')
            ])

        # è®¾ç½®åˆ—å®½
        ws.column_dimensions['A'].width = 8
        ws.column_dimensions['B'].width = 50
        ws.column_dimensions['C'].width = 20
        ws.column_dimensions['D'].width = 12
        ws.column_dimensions['E'].width = 10
        ws.column_dimensions['F'].width = 40

        # å†»ç»“é¦–è¡Œ
        ws.freeze_panes = 'A2'

    return wb


def create_excel_by_category(results):
    """æŒ‰åˆ†ç±»ç”Ÿæˆ Excel"""
    wb = Workbook()
    wb.remove(wb.active)

    # æŒ‰åˆ†ç±»åˆ†ç»„
    category_groups = {}
    for item in results:
        cat = item.get('category', 'å…¶ä»–')
        if cat not in category_groups:
            category_groups[cat] = []
        category_groups[cat].append(item)

    # åˆ†ç±»é¡ºåºå’Œ Sheet åç§°æ˜ å°„ï¼ˆå»æ‰ç‰¹æ®Šå­—ç¬¦ï¼‰
    category_mapping = {
        'å¤–æŒ‚ä½œå¼Š': 'å¤–æŒ‚ä½œå¼Š',
        'æ¸¸æˆä¼˜åŒ–': 'æ¸¸æˆä¼˜åŒ–',
        'æ¸¸æˆBug': 'æ¸¸æˆBug',
        'å……å€¼é€€æ¬¾': 'å……å€¼é€€æ¬¾',
        'æ–°æ¨¡å¼/åœ°å›¾/å¹³è¡¡æ€§å»ºè®®': 'æ–°æ¨¡å¼åœ°å›¾å¹³è¡¡æ€§å»ºè®®'  # å»æ‰æ–œæ 
    }

    # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»º Sheet
    for cat, sheet_name in category_mapping.items():
        if cat not in category_groups:
            continue

        items = category_groups[cat]
        ws = wb.create_sheet(title=sheet_name)

        # è¡¨å¤´
        headers = ['åºå·', 'åŸå§‹è¯„è®º', 'å½’ç±»', 'æƒ…æ„Ÿå€¾å‘', 'è¯­è¨€', 'ç®€è¦åˆ†æ']
        ws.append(headers)

        # è®¾ç½®è¡¨å¤´æ ·å¼
        header_fill = PatternFill(start_color='D32F2F', end_color='D32F2F', fill_type='solid')
        header_font = Font(bold=True, color='FFFFFF')
        for cell in ws[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center')

        # æ·»åŠ æ•°æ®
        for idx, item in enumerate(items, 1):
            ws.append([
                idx,
                item.get('text', ''),
                item.get('category', ''),
                item.get('sentiment', ''),
                item.get('language', ''),
                item.get('analysis', '')
            ])

        # è®¾ç½®åˆ—å®½
        ws.column_dimensions['A'].width = 8
        ws.column_dimensions['B'].width = 50
        ws.column_dimensions['C'].width = 20
        ws.column_dimensions['D'].width = 12
        ws.column_dimensions['E'].width = 10
        ws.column_dimensions['F'].width = 40

        # å†»ç»“é¦–è¡Œ
        ws.freeze_panes = 'A2'

    return wb


@app.route('/export_by_language')
@login_required
def export_by_language():
    """æŒ‰è¯­è¨€å¯¼å‡º Excel

    ä¼˜å…ˆæ ¹æ®å‰ç«¯ä¼ å…¥çš„ record_id å¯¼å‡ºã€Œå½“å‰æŸ¥çœ‹çš„é‚£ä¸€æ¡ã€å†å²è®°å½•ï¼›
    è‹¥æœªæä¾› record_idï¼Œåˆ™é€€å›åˆ°æ—§é€»è¾‘ï¼šå¯¼å‡ºå½“å‰ä¼šè¯æœ€è¿‘ä¸€æ¬¡åˆ†æç»“æœã€‚
    """
    user_id = session.get('user_id')
    record_id = request.args.get('record_id', type=int)

    results = []

    try:
        if record_id:
            # æ–°é€»è¾‘ï¼šæŒ‰è®°å½• ID ç²¾ç¡®å¯¼å‡ºå½“å‰æŸ¥çœ‹çš„å†å²è®°å½•
            logger.info(f"ğŸ“¥ æŒ‰è®°å½•IDå¯¼å‡ºè¯­è¨€åˆ†ç±»æŠ¥å‘Š: record_id={record_id}, user_id={user_id}")
            record = db.query_one("""
                SELECT result_json
                FROM analysis_results
                WHERE id = %s AND user_id = %s AND type = %s
            """, (record_id, user_id, 'sentiment'))

            if not record:
                return jsonify({'error': 'è®°å½•ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'}), 404

            if ANALYSIS_RESULTS_HAS_JSON and record.get('result_json'):
                try:
                    results = json.loads(record['result_json'])
                except Exception as e:
                    logger.error(f"âŒ è§£æ result_json å¤±è´¥: {e}")
                    return jsonify({'error': 'è¯¥è®°å½•çš„åŸå§‹æ•°æ®å·²æŸåï¼Œæ— æ³•å¯¼å‡º'}), 500
            else:
                return jsonify({'error': 'è¯¥å†å²è®°å½•ç”Ÿæˆäºæ—§ç‰ˆæœ¬ï¼Œæš‚ä¸æ”¯æŒå¯¼å‡ºï¼Œè¯·é‡æ–°åˆ†æä¸€æ¬¡'}), 400
        else:
            # å…¼å®¹æ—§é€»è¾‘ï¼šæŒ‰å½“å‰ä¼šè¯æœ€è¿‘ä¸€æ¬¡åˆ†æå¯¼å‡º
            session_id = session.get('session_id', 'default')
            results = LATEST_ANALYSIS_RESULTS.get(session_id, [])
            logger.info(f"ğŸ“¥ æŒ‰ä¼šè¯å¯¼å‡ºè¯­è¨€åˆ†ç±»æŠ¥å‘Š: session_id={session_id}, count={len(results)}")

        if not results:
            return jsonify({'error': 'æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®'}), 400

        wb = create_excel_by_language(results)

        # ç”Ÿæˆæ–‡ä»¶
        output = BytesIO()
        wb.save(output)
        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
        filename = f'è¯­è¨€åˆ†ç±»æŠ¥å‘Š_{timestamp}.xlsx'

        logger.info(f"ğŸ“¥ å¯¼å‡ºè¯­è¨€åˆ†ç±»æŠ¥å‘Š: {len(results)} æ¡æ•°æ®")

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=filename
        )
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/export_by_category')
@login_required
def export_by_category():
    """æŒ‰åˆ†ç±»å¯¼å‡º Excel

    ä¼˜å…ˆæ ¹æ®å‰ç«¯ä¼ å…¥çš„ record_id å¯¼å‡ºã€Œå½“å‰æŸ¥çœ‹çš„é‚£ä¸€æ¡ã€å†å²è®°å½•ï¼›
    è‹¥æœªæä¾› record_idï¼Œåˆ™é€€å›åˆ°æ—§é€»è¾‘ï¼šå¯¼å‡ºå½“å‰ä¼šè¯æœ€è¿‘ä¸€æ¬¡åˆ†æç»“æœã€‚
    """
    user_id = session.get('user_id')
    record_id = request.args.get('record_id', type=int)

    results = []

    try:
        if record_id:
            # æ–°é€»è¾‘ï¼šæŒ‰è®°å½• ID ç²¾ç¡®å¯¼å‡ºå½“å‰æŸ¥çœ‹çš„å†å²è®°å½•
            logger.info(f"ğŸ“¥ æŒ‰è®°å½•IDå¯¼å‡ºé—®é¢˜åˆ†ç±»æŠ¥å‘Š: record_id={record_id}, user_id={user_id}")
            record = db.query_one("""
                SELECT result_json
                FROM analysis_results
                WHERE id = %s AND user_id = %s AND type = %s
            """, (record_id, user_id, 'sentiment'))

            if not record:
                return jsonify({'error': 'è®°å½•ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®'}), 404

            if ANALYSIS_RESULTS_HAS_JSON and record.get('result_json'):
                try:
                    results = json.loads(record['result_json'])
                except Exception as e:
                    logger.error(f"âŒ è§£æ result_json å¤±è´¥: {e}")
                    return jsonify({'error': 'è¯¥è®°å½•çš„åŸå§‹æ•°æ®å·²æŸåï¼Œæ— æ³•å¯¼å‡º'}), 500
            else:
                return jsonify({'error': 'è¯¥å†å²è®°å½•ç”Ÿæˆäºæ—§ç‰ˆæœ¬ï¼Œæš‚ä¸æ”¯æŒå¯¼å‡ºï¼Œè¯·é‡æ–°åˆ†æä¸€æ¬¡'}), 400
        else:
            # å…¼å®¹æ—§é€»è¾‘ï¼šæŒ‰å½“å‰ä¼šè¯æœ€è¿‘ä¸€æ¬¡åˆ†æå¯¼å‡º
            session_id = session.get('session_id', 'default')
            results = LATEST_ANALYSIS_RESULTS.get(session_id, [])
            logger.info(f"ğŸ“¥ æŒ‰ä¼šè¯å¯¼å‡ºé—®é¢˜åˆ†ç±»æŠ¥å‘Š: session_id={session_id}, count={len(results)}")

        if not results:
            return jsonify({'error': 'æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®'}), 400

        wb = create_excel_by_category(results)

        # ç”Ÿæˆæ–‡ä»¶
        output = BytesIO()
        wb.save(output)
        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
        filename = f'é—®é¢˜åˆ†ç±»æŠ¥å‘Š_{timestamp}.xlsx'

        logger.info(f"ğŸ“¥ å¯¼å‡ºé—®é¢˜åˆ†ç±»æŠ¥å‘Š: {len(results)} æ¡æ•°æ®")

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=filename
        )
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# ============================================
# ç»Ÿè®¡åŠŸèƒ½
# ============================================

@app.route('/my-stats')
@login_required
def my_stats():
    """ä¸ªäººç»Ÿè®¡é¡µé¢"""
    user_id = session.get('user_id')

    # è·å–æœ¬æœˆç»Ÿè®¡æ•°æ®
    current_month = datetime.datetime.now().strftime('%Y-%m')

    stats_data = db.query_one("""
        SELECT
            COUNT(*) as count,
            COALESCE(SUM(comments_count), 0) as comments,
            COALESCE(SUM(total_cost), 0) as cost
        FROM usage_logs
        WHERE user_id = %s
          AND TO_CHAR(created_at, 'YYYY-MM') = %s
    """, (user_id, current_month))

    stats = {
        'count': stats_data['count'] if stats_data else 0,
        'comments': stats_data['comments'] if stats_data else 0,
        'cost': float(stats_data['cost']) if stats_data else 0.0,
        'avg_cost': float(stats_data['cost']) / stats_data['count'] if stats_data and stats_data['count'] > 0 else 0.0
    }

    # è·å–æœ€è¿‘20æ¡ä½¿ç”¨è®°å½•
    logs = db.query_all("""
        SELECT *
        FROM usage_logs
        WHERE user_id = %s
        ORDER BY created_at DESC
        LIMIT 20
    """, (user_id,))

    return render_template('my_stats.html', stats=stats, logs=logs, user=session)


@app.route('/admin')
@admin_required
def admin_panel():
    """ç®¡ç†åå°é¡µé¢"""
    current_month = datetime.datetime.now().strftime('%Y-%m')

    # å…¨å±€ç»Ÿè®¡
    global_data = db.query_one("""
        SELECT
            COALESCE(SUM(total_cost), 0) as total_cost,
            COUNT(DISTINCT user_id) as active_users,
            COUNT(*) as total_count
        FROM usage_logs
        WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
    """, (current_month,))

    global_stats = {
        'total_cost': float(global_data['total_cost']) if global_data else 0.0,
        'active_users': global_data['active_users'] if global_data else 0,
        'total_count': global_data['total_count'] if global_data else 0,
        'avg_cost': float(global_data['total_cost']) / global_data['total_count'] if global_data and global_data['total_count'] > 0 else 0.0
    }

    # éƒ¨é—¨ç»Ÿè®¡
    dept_stats = db.query_all("""
        SELECT
            department,
            COALESCE(SUM(total_cost), 0) as cost,
            COUNT(*) as count
        FROM usage_logs
        WHERE TO_CHAR(created_at, 'YYYY-MM') = %s
        GROUP BY department
        ORDER BY cost DESC
    """, (current_month,))

    # ç”¨æˆ·ç»Ÿè®¡ï¼ˆTop 10ï¼‰
    user_stats = db.query_all("""
        SELECT
            u.real_name,
            u.department,
            COALESCE(SUM(l.total_cost), 0) as cost,
            COUNT(l.id) as count
        FROM users u
        LEFT JOIN usage_logs l ON u.id = l.user_id
            AND TO_CHAR(l.created_at, 'YYYY-MM') = %s
        GROUP BY u.id, u.real_name, u.department
        ORDER BY cost DESC
        LIMIT 10
    """, (current_month,))

    # æ‰€æœ‰ç”¨æˆ·åˆ—è¡¨
    all_users = db.query_all("""
        SELECT * FROM users ORDER BY created_at DESC
    """)

    return render_template('admin.html',
                         global_stats=global_stats,
                         dept_stats=dept_stats,
                         user_stats=user_stats,
                         all_users=all_users,
                         user=session)


@app.route('/admin/add_user', methods=['POST'])
@admin_required
def add_user():
    """æ·»åŠ æ–°ç”¨æˆ·"""
    try:
        data = request.json
        username = data.get('username')
        password = data.get('password')
        real_name = data.get('real_name')
        department = data.get('department')
        role = data.get('role', 'user')

        # æ£€æŸ¥ç”¨æˆ·åæ˜¯å¦å·²å­˜åœ¨
        existing = db.query_one("SELECT id FROM users WHERE username = %s", (username,))
        if existing:
            return jsonify({'error': 'ç”¨æˆ·åå·²å­˜åœ¨'}), 400

        # åŠ å¯†å¯†ç 
        password_hash = bcrypt.generate_password_hash(password).decode('utf-8')

        # æ’å…¥ç”¨æˆ·
        db.execute("""
            INSERT INTO users (username, password_hash, real_name, department, role)
            VALUES (%s, %s, %s, %s, %s)
        """, (username, password_hash, real_name, department, role))

        logger.info(f"âœ… ç®¡ç†å‘˜æ·»åŠ æ–°ç”¨æˆ·: {username} ({real_name})")

        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"âŒ æ·»åŠ ç”¨æˆ·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/admin/delete_user/<int:user_id>', methods=['DELETE'])
@admin_required
def delete_user(user_id):
    """åˆ é™¤ç”¨æˆ·"""
    try:
        # ä¸å…è®¸åˆ é™¤ç®¡ç†å‘˜è´¦å·
        user = db.query_one("SELECT username FROM users WHERE id = %s", (user_id,))
        if user and user['username'] == 'admin':
            return jsonify({'error': 'ä¸èƒ½åˆ é™¤ç®¡ç†å‘˜è´¦å·'}), 403

        # åˆ é™¤ç”¨æˆ·
        db.execute("DELETE FROM users WHERE id = %s", (user_id,))

        logger.info(f"âœ… ç®¡ç†å‘˜åˆ é™¤ç”¨æˆ·: ID={user_id}")

        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ç”¨æˆ·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


# ============================================
# åº”ç”¨å¯åŠ¨
# ============================================

if __name__ == '__main__':
    # æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡
    logger.info("ğŸ”„ æ£€æŸ¥å¹¶æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡...")
    recover_interrupted_tasks()

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ Sailson AI å·¥ä½œå°å·²å¯åŠ¨")
    logger.info(f"ğŸŒ è®¿é—®åœ°å€: http://0.0.0.0:{PORT}")
    logger.info("=" * 60 + "\n")

    app.run(debug=False, host='0.0.0.0', port=PORT)
